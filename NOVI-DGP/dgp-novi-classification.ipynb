{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import math\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.variational import VariationalStrategy\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:6' # change here to tune the device we use\n",
    "else:\n",
    "    device = 'cpu'\n",
    "dataset = 'cifar10' # set here to determine to train which dataset\n",
    "if dataset == 'red_wine':\n",
    "    # L=6, D=11, N=1599\n",
    "    data = pd.read_csv('data/red_wine.csv').values\n",
    "    X = torch.Tensor(data[:, :11])\n",
    "    y = torch.Tensor(data[:, 11]).add(-3).long()\n",
    "elif dataset == 'white_wine':\n",
    "    # L=7, D=11, N=4898\n",
    "    data = pd.read_csv('data/white_wine.csv').values\n",
    "    X = torch.Tensor(data[:, :11])\n",
    "    y = torch.Tensor(data[:, 11]).add(-3).long()\n",
    "elif dataset == 'cifar10':\n",
    "    # L=10, C=3. H=32, W=32\n",
    "    pass\n",
    "elif dataset == 'mnist':\n",
    "    # L=10, C=1, H=28, W=28, label: 0-9\n",
    "    pass\n",
    "elif dataset == 'fashion_mnist':\n",
    "    # L=10, C=1, H=28, W=28, label: 0-9\n",
    "    pass\n",
    "elif dataset == 'abalone':\n",
    "    # L=29, D=7, N=4177\n",
    "    data = pd.read_excel('data/abalone.xlsx', header=None).values\n",
    "    X = torch.Tensor(data[:, :8])\n",
    "    y = torch.Tensor(data[:, 8]).add(-1).long()\n",
    "elif dataset == 'wilt':\n",
    "    # L=2, D=5, N=4840\n",
    "    train_data = pd.read_csv('data/wilt_training.csv').values\n",
    "    test_data = pd.read_csv('data/wilt_test.csv').values\n",
    "    train_x = train_data[:, 1:]\n",
    "    test_x = test_data[:, 1:]\n",
    "    train_x = train_x - train_x.min(0)[0]  # X.min(0)[0]: min value of every feature\n",
    "    train_x = 2 * (train_x / train_x.max(0)[0]) - 1 # pre-preocess to [-1,1]\n",
    "    test_x = test_x - test_x.min(0)[0]  # X.min(0)[0]: min value of every feature\n",
    "    test_x = 2 * (test_x / test_x.max(0)[0]) - 1 # pre-preocess to [-1,1]\n",
    "    \n",
    "    train_x = torch.Tensor(train_x).contiguous()\n",
    "    test_x = torch.Tensor(test_x).contiguous()\n",
    "    train_y = torch.Tensor(train_data[:, 0]).long().contiguous()\n",
    "    test_y = torch.Tensor(test_data[:, 0]).long().contiguous()\n",
    "    \n",
    "# pre-processing\n",
    "if dataset == 'cifar10':\n",
    "    transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "elif dataset == 'mnist':\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(), \n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "elif dataset == 'fashion_mnist':\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(), \n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "def get_parameter_number(model):\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "if dataset == 'red_wine':\n",
    "    loader_batch_size = 512\n",
    "elif dataset == 'white_wine':\n",
    "    loader_batch_size = 512\n",
    "elif dataset == 'cifar10':\n",
    "    loader_batch_size = 512\n",
    "elif dataset == 'mnist':\n",
    "    loader_batch_size = 128\n",
    "elif dataset == 'fashion_mnist':\n",
    "    loader_batch_size = 256\n",
    "elif dataset == 'abalone':\n",
    "    loader_batch_size = 1024\n",
    "elif dataset == 'wilt':\n",
    "    loader_batch_size = 1024\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, \n",
    "                                                 transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, \n",
    "                                                 transform=transform_test)\n",
    "elif dataset == 'mnist':\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=False, \n",
    "                                                 transform=transform)\n",
    "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=False, \n",
    "                                                 transform=transform)\n",
    "elif dataset == 'fashion_mnist':\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, \n",
    "                                                 transform=transform)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True,  transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader =  DataLoader(test_dataset, batch_size=256)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader1 = DataLoader(train_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_time = False # set True to print module time\n",
    "print_norm = False # set True to print norm of f_u (the perfect value of norm after training: 0)\n",
    "print_metric = True # set True to print metric per epoch\n",
    "print_param = True # set True to print parameter size of each network\n",
    "print_loss = False # set True to print each loss during training\n",
    "sample_times = 1 # Sample k times to evaluate the expectation over q(u)\n",
    "concat_type = False # set True to concat e with z in generator G\n",
    "expect_mean = True # Set True to compute mean of q(f) by average over q(u)\n",
    "\n",
    "class ToyDeepGPHiddenLayer(DeepGPLayer):  # input_dims: size of feature dim\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=128, mean_type='constant', stein=False, noise_add=True,\n",
    "                 noise_share=False, noise_dim=1, multi_head=False, vector_type=False):\n",
    "        # TODO: adjust the size of inducing_points to [inducing_size, input_dim] for each layer\n",
    "        if stein is False:\n",
    "            if output_dims is None:\n",
    "                inducing_points = torch.randn((num_inducing, input_dims), requires_grad=True) # sample from gaussian dist\n",
    "                batch_shape = torch.Size([])\n",
    "            else: # inducing_points-dim correspond to: output_dims, num_inducing and input_dims\n",
    "                inducing_points = torch.randn((output_dims, num_inducing, input_dims), requires_grad=True)\n",
    "                batch_shape = torch.Size([output_dims])\n",
    "        else:\n",
    "            if output_dims is None:\n",
    "                inducing_points = torch.randn((num_inducing, input_dims), requires_grad=True) # sample from gaussian dist\n",
    "                batch_shape = torch.Size([])\n",
    "            else: # inducing_points-dim correspond to: [num_inducing, input_dims]\n",
    "                inducing_points = torch.randn((num_inducing, input_dims), requires_grad=True)\n",
    "                batch_shape = torch.Size([output_dims])\n",
    "        \n",
    "        self.input_dim = input_dims\n",
    "        self.output_dim = output_dims\n",
    "        self.inducing_size = num_inducing\n",
    "\n",
    "        self.noise_add = noise_add\n",
    "        self.noise_share = noise_share\n",
    "        self.noise_dim = noise_dim # get this value from the outer model\n",
    "        if self.noise_dim is None:\n",
    "            self.noise_dim = 32\n",
    "        self.noise_add = noise_add\n",
    "        if self.noise_add:\n",
    "            self.concat_type = concat_type\n",
    "        else:\n",
    "            self.concat_type = False # only can set True when adding noise\n",
    "\n",
    "        # self.share_noise = share_noise # noise shared by all layers\n",
    "        self.multi_head = multi_head\n",
    "        # self.transformed_noise = transformed_noise\n",
    "        self.learn_inducing_locations = True # Set True to treat location of z as parameter\n",
    "        \n",
    "        self.hutch_times = 1 # number for hutchison estimation of trace\n",
    "        self.bottleneck_trick = True # Set True to introduce decomposition of jacobian to reduce variance\n",
    "        self.rademacher_type = False # Set True to sample from Rademecher dist to compute trace\n",
    "        self.diagonal_type = True # Set True to use diagonal kernel matrix to approximate fully kernel matrix\n",
    "        self.vector_type = vector_type # Set True to generate u using vector-based network\n",
    "        \n",
    "        self.print_time = print_time\n",
    "        self.print_param = print_param\n",
    "        self.device = device\n",
    "        self.sample_times = sample_times # Sample k times to evaluate the expectation over q(u)\n",
    "        self.expect_mean = expect_mean\n",
    "        \n",
    "        variational_distribution = None\n",
    "\n",
    "        variational_strategy = VariationalStrategy( # transform q(U) to q(F)\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            stein_type=stein,\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "\n",
    "        super(ToyDeepGPHiddenLayer, self).__init__(variational_strategy, input_dims, output_dims)        \n",
    "        self.variational_strategy = variational_strategy\n",
    "\n",
    "    def forward(self, x):\n",
    "        return None\n",
    "    \n",
    "    # will be called when run the forward function of class DeepGP\n",
    "    def __call__(self, x, train_type=True, disc_type=True, hyp_type=True, trace_layer_list=None, norm_layer_list=None, u_layer_list=None,\n",
    "                 fiu_layer_list=None, glogpfu_layer_list=None, shared_list=None, transformed_list=None, *other_inputs,\n",
    "                 **kwargs): \n",
    "        \"\"\"\n",
    "        Overriding __call__ isn't strictly necessary, but it lets us add concatenation based skip connections\n",
    "        easily.\n",
    "        \"\"\"\n",
    "        return super().__call__(x, are_samples=False, train_type=train_type, disc_type=disc_type,\n",
    "                                hyp_type=hyp_type, trace_layer_list=trace_layer_list, norm_layer_list=norm_layer_list,\n",
    "                                u_layer_list=u_layer_list, fiu_layer_list=fiu_layer_list, \n",
    "                                glogpfu_layer_list=glogpfu_layer_list, shared_list=shared_list,\n",
    "                               transformed_list=transformed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_dims = 10  # number of output_dim in hidden layer\n",
    "vector_type = True # Set True to generate u using vector-based network\n",
    "learn_likelihood_covariance = True # Set True to treat the covariance of likelihood as parameter\n",
    "if dataset == 'cifar10':\n",
    "    task_dim = 10\n",
    "    train_x_shape = 64 # carefully examine this term if self.feature_extractor is modified\n",
    "elif dataset == 'mnist':\n",
    "    task_dim = 10\n",
    "    train_x_shape = 28 * 28 # carefully examine this term if self.feature_extractor is modified\n",
    "elif dataset == 'fashion_mnist':\n",
    "    task_dim = 10\n",
    "    train_x_shape = 8 * 8 # carefully examine this term if self.feature_extractor is modified\n",
    "elif dataset == 'red_wine':\n",
    "    task_dim = 6\n",
    "    train_x_shape = train_x.shape[-1]\n",
    "elif dataset == 'white_wine':\n",
    "    task_dim = 7\n",
    "    train_x_shape = train_x.shape[-1]\n",
    "elif dataset == 'abalone':\n",
    "    task_dim = 29\n",
    "    train_x_shape = train_x.shape[-1]\n",
    "elif dataset == 'wilt':\n",
    "    task_dim = 2\n",
    "    train_x_shape = train_x.shape[-1]\n",
    "        \n",
    "# we can fine-tune it to obtain a better result\n",
    "if dataset == 'red_wine':\n",
    "    noise_dim = 200\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'white_wine':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'cifar10':\n",
    "    noise_dim = 200\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'mnist':\n",
    "    noise_dim = 200\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'fashion_mnist':\n",
    "    noise_dim = 200\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'abalone':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'wilt':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "\n",
    "\n",
    "class ResNet18FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=64):\n",
    "        super(ResNet18FeatureExtractor, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False)\n",
    "        self.resnet.maxpool = nn.MaxPool2d(1, 1, 0)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.resnet.fc(x)\n",
    "        return x\n",
    "Extractor = ResNet18FeatureExtractor().to(device)\n",
    "state_dict = torch.load('ResNet_trained_extractor.pth')\n",
    "Extractor.load_state_dict(state_dict)\n",
    "\n",
    "        \n",
    "class DeepGP(DeepGP): # define the noise outside the layer\n",
    "    def __init__(self, train_x_shape, stein=False, noise_add=True, noise_share=False, multi_head=False): # L=2\n",
    "        if noise_add is False and noise_share:\n",
    "            raise ValueError('Only can share noise across layer when noise is added')\n",
    "        if noise_share is False and multi_head:\n",
    "            raise ValueError('Only can use Multi-head Mechanism when noise is shared across layer')\n",
    "        super().__init__()\n",
    "        self.vector_type = vector_type\n",
    "        self.noise_add = noise_add\n",
    "        self.noise_share = noise_share # share the noise across layer or not\n",
    "        # fine-tune this term or directly learn as a prior\n",
    "        self.noise_dim = noise_dim\n",
    "        self.multi_head = multi_head  # transform noise or not        \n",
    "        self.back_bone = nn.Sequential(\n",
    "            nn.Linear(in_features=self.noise_dim, out_features=32),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=32, out_features=self.noise_dim)\n",
    "        )\n",
    "        if self.multi_head and print_param:\n",
    "            print('Generator backbone:', get_parameter_number(self.back_bone))\n",
    "        \n",
    "        lls_sigma = torch.Tensor([0.1]) # Set sigma to be a small number\n",
    "        if learn_likelihood_covariance:\n",
    "            self.register_parameter(\"lls_sigma\", torch.nn.Parameter(lls_sigma))\n",
    "        else:\n",
    "            self.register_buffer(\"lls_sigma\", lls_sigma)\n",
    "        \n",
    "        if dataset == 'cifar10': # customize a feature_extractor for each image dataset\n",
    "           \n",
    "            with torch.no_grad():\n",
    "                self.feature_extractor = Extractor.eval().to(device)\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "                #self.feature_extractor = ResNetFeatureExtractor.eval().to(device)\n",
    "            #self.feature_extractor=FeatureExtractor()\n",
    "        elif dataset == 'mnist' or 'fashion_mnist':\n",
    "            self.feature_extractor = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten(), # output_dim: 28 * 28\n",
    "            )\n",
    "        elif dataset == 'red_wine' or 'white_wine' or 'abalone' or 'wilt':\n",
    "            self.feature_extractor = nn.Flatten()\n",
    "\n",
    "        self.post_classification = nn.Sequential(\n",
    "            #nn.Linear(in_features=task_dim, out_features=task_dim),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.Linear(in_features=task_dim, out_features=task_dim),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.cross = nn.CrossEntropyLoss() # compute the classification loss to optimize hyper-parameter\n",
    "        \n",
    "        hidden_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=train_x_shape,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "        '''\n",
    "        hidden_layer2 = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer.output_dim,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )   \n",
    "        \n",
    "        \n",
    "        hidden_layer3 = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer2.output_dim,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        hidden_layer4 = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer3.output_dim,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "        '''\n",
    "\n",
    "        last_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer.output_dim,\n",
    "            output_dims=task_dim,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='constant',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "\n",
    "        self.hidden_layer = hidden_layer\n",
    "        #self.hidden_layer2 = hidden_layer2\n",
    "        #self.hidden_layer3 = hidden_layer3\n",
    "        # self.hidden_layer4 = hidden_layer4\n",
    "        self.last_layer = last_layer\n",
    "        \n",
    "        # register all networks here!!!\n",
    "        self.hidden_strategy_generator = self.hidden_layer.variational_strategy.generator\n",
    "        self.hidden_strategy_discriminator = self.hidden_layer.variational_strategy.discriminator\n",
    "        self.hidden_kernel_method = self.hidden_layer.variational_strategy.kernel_method\n",
    "        '''\n",
    "        self.hidden2_strategy_generator = self.hidden_layer2.variational_strategy.generator\n",
    "        self.hidden2_strategy_discriminator = self.hidden_layer2.variational_strategy.discriminator\n",
    "        self.hidden2_kernel_method = self.hidden_layer2.variational_strategy.kernel_method\n",
    "        \n",
    "        self.hidden3_strategy_generator = self.hidden_layer3.variational_strategy.generator\n",
    "        self.hidden3_strategy_discriminator = self.hidden_layer3.variational_strategy.discriminator\n",
    "        self.hidden3_kernel_method = self.hidden_layer3.variational_strategy.kernel_method\n",
    "        \n",
    "        self.hidden4_strategy_generator = self.hidden_layer4.variational_strategy.generator\n",
    "        self.hidden4_strategy_discriminator = self.hidden_layer4.variational_strategy.discriminator\n",
    "        self.hidden4_kernel_method = self.hidden_layer4.variational_strategy.kernel_method\n",
    "        '''\n",
    "        self.last_strategy_generator = self.last_layer.variational_strategy.generator\n",
    "        self.last_strategy_discriminator = self.last_layer.variational_strategy.discriminator\n",
    "        self.last_kernel_method = self.last_layer.variational_strategy.kernel_method\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # set train_type=False when evaluate, do not compute the loss to faster the process\n",
    "        # Generate noise here when noise is shared or transformed\n",
    "        if self.noise_add:\n",
    "            if self.noise_share:\n",
    "                if self.multi_head:\n",
    "                    if self.vector_type:\n",
    "                        shared_list = []\n",
    "                        transformed_list = []\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = 0.1*torch.randn(self.noise_dim, requires_grad=True).to(device)\n",
    "                            transformed_noise = self.back_bone(shared_noise)\n",
    "                            shared_list.append(shared_noise)\n",
    "                            transformed_list.append(transformed_noise)\n",
    "                    else:\n",
    "                        shared_list = []\n",
    "                        transformed_list = []\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = torch.randn((num_layer_inducing, self.noise_dim), requires_grad=True).to(device)\n",
    "                            transformed_noise = self.back_bone(shared_noise)\n",
    "                            shared_list.append(shared_noise)\n",
    "                            transformed_list.append(transformed_noise)\n",
    "                else:\n",
    "                    if self.vector_type:\n",
    "                        shared_list = []\n",
    "                        transformed_list = None\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = torch.randn(self.noise_dim, requires_grad=True).to(device)\n",
    "                            shared_list.append(shared_noise)                        \n",
    "                    else:\n",
    "                        shared_list = []\n",
    "                        transformed_list = None\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = torch.randn((num_layer_inducing, self.noise_dim), requires_grad=True).to(device)\n",
    "                            shared_list.append(shared_noise)  \n",
    "            else:\n",
    "                shared_list = None\n",
    "                transformed_list = None\n",
    "        else:\n",
    "            shared_list = None\n",
    "            transformed_list = None\n",
    "        inputs, train_type, disc_type, hyp_type, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = inputs\n",
    "        # determine to train backbone network and parameter sigma or not\n",
    "        if train_type:\n",
    "            if disc_type:\n",
    "                self.back_bone.requires_grad_(False)\n",
    "                self.lls_sigma.requires_grad_(False)\n",
    "                self.feature_extractor.requires_grad_(False)\n",
    "                self.post_classification.requires_grad_(False)\n",
    "            else:\n",
    "                self.back_bone.requires_grad_(True)\n",
    "                self.lls_sigma.requires_grad_(False)\n",
    "                self.feature_extractor.requires_grad_(False)\n",
    "                self.post_classification.requires_grad_(False)\n",
    "        else:\n",
    "            if hyp_type:\n",
    "                self.back_bone.requires_grad_(False)\n",
    "                self.lls_sigma.requires_grad_(True)\n",
    "                self.feature_extractor.requires_grad_(False)\n",
    "                self.post_classification.requires_grad_(True)\n",
    "            else: # use for test stage\n",
    "                self.back_bone.requires_grad_(False)\n",
    "                self.lls_sigma.requires_grad_(False)\n",
    "                self.feature_extractor.requires_grad_(False)\n",
    "                self.post_classification.requires_grad_(False)\n",
    "        \n",
    "        hidden_rep1, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer(\n",
    "                                                                            inputs, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        '''\n",
    "        hidden_rep2, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer2(\n",
    "                                                                            hidden_rep1, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        \n",
    "        hidden_rep3, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer3(\n",
    "                                                                            hidden_rep2, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        \n",
    "        hidden_rep4, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer4(\n",
    "                                                                            hidden_rep3, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        '''\n",
    "        \n",
    "        # get a prob distribution, not a deterministic vector value\n",
    "        output = self.last_layer(hidden_rep1, train_type=train_type, disc_type=disc_type, hyp_type=hyp_type, \n",
    "                                 trace_layer_list=trace_layer_list, norm_layer_list=norm_layer_list, \n",
    "                                 u_layer_list=u_layer_list, fiu_layer_list=fiu_layer_list, \n",
    "                                 glogpfu_layer_list=glogpfu_layer_list, shared_list=shared_list, \n",
    "                                 transformed_list=transformed_list)\n",
    "        return output\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        correct_sum = 0\n",
    "        total_sum = 0\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            x_batch = self.feature_extractor.forward(x_batch)\n",
    "            output_batch, _, _, _, _, _ = self.forward((x_batch, False, False, True, [], [], [], [], []))\n",
    "            mean_batch, covar_batch = output_batch\n",
    "            f_batch_list = []\n",
    "            for t in range(0, task_dim):\n",
    "                f_batch = torch.distributions.MultivariateNormal(loc=mean_batch[t, :], covariance_matrix=covar_batch[t, :, :]).rsample(\n",
    "                        torch.Size([])) # sample f from q(f)\n",
    "                f_batch_list.append(f_batch)\n",
    "            f_batch_total = torch.stack(f_batch_list, dim=1) # size: [batch_size, task_dim]\n",
    "            preds = self.classification_likelihood(f_batch_total)\n",
    "            #print(preds)\n",
    "            #print(y_batch)\n",
    "            correct = (preds == y_batch).sum().item()\n",
    "            total = y_batch.size(0)\n",
    "            print('Batch Accuracy: {}%'.format(100 * correct / total))\n",
    "\n",
    "            correct_sum += correct\n",
    "            total_sum += total\n",
    "                \n",
    "\n",
    "        total_acc = 100 * correct_sum / total_sum \n",
    "        return total_acc\n",
    "    \n",
    "    def classification_likelihood_loss(self, y_batch, output_batch, u_layer_list, fiu_layer_list, glogpfu_layer_list):\n",
    "        mean_batch, covariance_batch = output_batch\n",
    "        f_batch_list = []\n",
    "        for t in range(0, task_dim):\n",
    "            f_batch = torch.distributions.MultivariateNormal(loc=mean_batch[t, :], \n",
    "                                                             covariance_matrix=covariance_batch[t, :, :]).rsample(torch.Size([]))\n",
    "            f_batch_list.append(f_batch)\n",
    "        f_batch_total = torch.stack(f_batch_list, dim=1) # size: [batch_size, task_dim]\n",
    "        point_batch = self.post_classification(f_batch_total) # size: [batch_size, task_dim]\n",
    "        noise_batch = torch.ones_like(f_batch_total).mul(1e-4).to(device) # remove it if training of classification is stable\n",
    "        point_batch = point_batch.add(noise_batch)\n",
    "        #f_batch_total = f_batch_total.add(noise_batch)\n",
    "        y_size = y_batch.size(0)\n",
    "        one_hot_batch = torch.zeros(y_size, task_dim).long().to(device)\n",
    "        one_hot_batch.scatter_(dim=1, \n",
    "                               index=y_batch.unsqueeze(dim=1), \n",
    "                               src=torch.ones(y_size, task_dim).long().to(device)) # [batch_size, task_size]\n",
    "        prob_batch = torch.matmul(point_batch.unsqueeze(1), one_hot_batch.float().unsqueeze(2))\n",
    "        prob_batch = prob_batch.squeeze(2).squeeze(1)\n",
    "        log_prob_batch = torch.log(prob_batch).sum(0) # get the final log-prob value\n",
    "        \n",
    "        glogpyf_layer_list = [] # use to contain gradient of likelihood\n",
    "        for l in range(0, len(u_layer_list)):\n",
    "            glogpyf_l_list = []\n",
    "            for k in range(0, sample_times):\n",
    "                if expect_mean:\n",
    "                    glogpyf_l = torch.autograd.grad(log_prob_batch, u_layer_list[l][k], retain_graph=True, create_graph=True)[0]\n",
    "                else:\n",
    "                    # Set expect_mean == False to generate q(f) only using the first element of u_layer_list\n",
    "                    glogpyf_l = torch.autograd.grad(log_prob_batch, u_layer_list[l][0], retain_graph=True, create_graph=True)[0]\n",
    "                glogpyf_l_list.append(glogpyf_l)\n",
    "            glogpyf_layer_list.append(glogpyf_l_list)\n",
    "        \n",
    "        glogpyffu_layer_list = []\n",
    "        # glogpyffu_layer_list: the second term in the first loss term in each layer\n",
    "        for l in range(0, len(u_layer_list)):\n",
    "            glogpyffu_l_list = []\n",
    "            for k in range(0, sample_times):\n",
    "                glogpyffu_l = torch.mm(glogpyf_layer_list[l][k].unsqueeze(0), fiu_layer_list[l][k].unsqueeze(1))\n",
    "                glogpyffu_l_list.append(glogpyffu_l)\n",
    "            glogpyffu_l_tensor = torch.stack(glogpyffu_l_list, dim=0)\n",
    "            glogpyffu_l_mean = glogpyffu_l_tensor.mean(0) # average over sample_times\n",
    "            glogpyffu_layer_list.append(glogpyffu_l_mean)\n",
    "        glogpyffu_tensor = torch.stack(glogpyffu_layer_list, dim=0)\n",
    "        glogpyffu_loss = glogpyffu_tensor.sum(0) # sum over all layers\n",
    "        \n",
    "        return glogpyffu_loss\n",
    "    \n",
    "    def classification_likelihood(self, f_batch):\n",
    "        # size of f_batch: [batch_size, task_dim]\n",
    "        # We return the integer of class number that we predict the image belongs to\n",
    "        #prob_batch = self.post_classification(f_batch)\n",
    "        #y_predict = torch.max(prob_batch, dim=1)[1] # return the index [0, L-1] of each data point\n",
    "        y_predict = torch.max(f_batch, dim=1)[1] # return the index [0, L-1] of each data point\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator backbone: {'Total': 13032, 'Trainable': 13032}\n",
      "Generator: {'Total': 56864, 'Trainable': 56864}\n",
      "Discriminator: {'Total': 42257, 'Trainable': 42257}\n",
      "Kernel method: {'Total': 66, 'Trainable': 66}\n",
      "Generator: {'Total': 49952, 'Trainable': 49952}\n",
      "Discriminator: {'Total': 42257, 'Trainable': 42257}\n",
      "Kernel method: {'Total': 12, 'Trainable': 12}\n",
      "Test whether you have cuda to run the process or not: True\n",
      "{'Total': 11406105, 'Trainable': 204441}\n"
     ]
    }
   ],
   "source": [
    "# NOTE: When strange error occur (especially wrong error line), restart the kernel\n",
    "# If multi_head=True, Transform shared noise by a network and concat with layer-specified inducing points\n",
    "model = DeepGP(train_x_shape, stein=True, noise_add=True, noise_share=True, multi_head=True)\n",
    "print('Test whether you have cuda to run the process or not:', torch.cuda.is_available())\n",
    "model = model.to(device)\n",
    "print(get_parameter_number(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_list = [] \n",
    "acc_train_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total': 11406105, 'Trainable': 204441}\n"
     ]
    }
   ],
   "source": [
    "print(get_parameter_number(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ca2633364a4a8e862c9d7684f4fb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New epoch!!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6824628a39d84b3da9e8008ad529f1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Discriminator:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c0c40b03d843af8cd34db82451ce9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Disc:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH DISC_ACC: 11.328125%\n",
      "MINI-BATCH DISC_ACC: 12.109375%\n",
      "MINI-BATCH DISC_ACC: 10.546875%\n",
      "MINI-BATCH DISC_ACC: 6.640625%\n",
      "MINI-BATCH DISC_ACC: 12.5%\n",
      "MINI-BATCH DISC_ACC: 10.9375%\n",
      "MINI-BATCH DISC_ACC: 8.984375%\n",
      "MINI-BATCH DISC_ACC: 9.765625%\n",
      "MINI-BATCH DISC_ACC: 7.8125%\n",
      "MINI-BATCH DISC_ACC: 12.109375%\n",
      "MINI-BATCH DISC_ACC: 11.328125%\n",
      "MINI-BATCH DISC_ACC: 11.328125%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34715/2848879049.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0mx_batch_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_disc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     output_batch_disc, trace_layer_list_disc, norm_layer_list_disc, u_layer_list_disc, fiu_layer_list_disc, glogpfu_layer_list_disc = model.forward(\n\u001b[0;32m---> 50\u001b[0;31m                         (x_batch_disc, True, True, False, trace_layer_list_disc, norm_layer_list_disc, u_layer_list_disc, fiu_layer_list_disc, glogpfu_layer_list_disc))\n\u001b[0m\u001b[1;32m     51\u001b[0m                     \u001b[0;31m# size of output:([1, batch_size], [1, batch_size, batch_size])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;31m# the second loss term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_34715/2119667523.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                  \u001b[0mu_layer_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mu_layer_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiu_layer_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiu_layer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                                  \u001b[0mglogpfu_layer_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglogpfu_layer_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshared_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                                  transformed_list=transformed_list)\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_34715/3988560266.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, train_type, disc_type, hyp_type, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list, shared_list, transformed_list, *other_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                                 \u001b[0mu_layer_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mu_layer_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiu_layer_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiu_layer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                                 \u001b[0mglogpfu_layer_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglogpfu_layer_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshared_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                                transformed_list=transformed_list)\n\u001b[0m",
      "\u001b[0;32m~/xujian/stein+dgp-reg+class/gpytorch/models/deep_gps/deep_gp.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, are_samples, train_type, disc_type, hyp_type, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list, shared_list, transformed_list, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m#     torch.Size([]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 input_i = torch.distributions.MultivariateNormal(loc=means[i, :],\n\u001b[0;32m---> 94\u001b[0;31m                                                                  \u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcovariances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                     torch.Size([]))\n\u001b[1;32m     96\u001b[0m                 \u001b[0minput_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xujian/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mevent_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale_tril\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xujian/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                     raise ValueError(\n\u001b[1;32m     56\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from gpytorch.settings import num_likelihood_samples\n",
    "begin_sum = time.time()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "optimizer1 = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.02)\n",
    "optimizer_gen = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "#def train(num_epochs=3, num_disc=2, num_gen=2, num_hyp=10, num_samples=10, lamda=100, lamda_like=1):\n",
    "\n",
    "num_epochs = 1\n",
    "num_disc = 1 # use to train discriminator per epoch\n",
    "num_gen = 1# use to train generator per epoch\n",
    "num_hyp = 10 # use to train hyper-parameter per epoch\n",
    "num_samples = 10 # use to evaluate likelihood\n",
    "lamda = 100 # hyperparameter for norm loss\n",
    "lamda_like = 1 # hyperparameter for likelihood loss (will be deleted soon)\n",
    "\n",
    "if num_epochs == 0:\n",
    "    raise ValueError('At least one epoch need to be run!!!')\n",
    "\n",
    "epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter: # change here to add adversarial training\n",
    "    print('New epoch!!!')\n",
    "    if num_disc != 0:\n",
    "        correct_epoch_disc = 0 # use to calculate correct prediction in one epoch\n",
    "        total_epoch_disc = 0 # use to calculate total prediction in one epoch\n",
    "        # fix hyperparameter v and generator G, update discriminator D num_nc times\n",
    "        disc_iter = tqdm.notebook.tqdm(range(num_disc), desc='Train Discriminator', leave=False)\n",
    "        for j in disc_iter:\n",
    "            # Within each iteration, we will go over each minibatch of data\n",
    "            minibatch_iter_disc = tqdm.notebook.tqdm(train_loader1, desc=\"Minibatch-Disc\", leave=False)\n",
    "            correct_iter_disc = 0 # use to calculate correct prediction in one iteration\n",
    "            total_iter_disc = 0 # use to calculate total prediction in one iteration\n",
    "            for x_batch_disc, y_batch_disc in minibatch_iter_disc:\n",
    "                x_batch_disc, y_batch_disc = x_batch_disc.to(device), y_batch_disc.to(device)\n",
    "                with num_likelihood_samples(num_samples):\n",
    "                    optimizer.zero_grad()\n",
    "                    trace_layer_list_disc = []\n",
    "                    norm_layer_list_disc = []\n",
    "                    u_layer_list_disc = []\n",
    "                    fiu_layer_list_disc = []\n",
    "                    glogpfu_layer_list_disc = []\n",
    "                    # trace_layer_list: contain the final trace in each layer\n",
    "                    # norm_layer_list: contain the norm in each layer\n",
    "                    # u_layer_list: contain u in each layer\n",
    "                    # fiu_layer_list: contain f_u in each layer\n",
    "                    # glogpu_layer_list: contain dot product of gradient of logp_u with f_u in each layer\n",
    "                    # disc_type: determine update discriminator or generator\n",
    "                    x_batch_disc = model.feature_extractor.forward(x_batch_disc)\n",
    "                    output_batch_disc, trace_layer_list_disc, norm_layer_list_disc, u_layer_list_disc, fiu_layer_list_disc, glogpfu_layer_list_disc = model.forward(\n",
    "                        (x_batch_disc, True, True, False, trace_layer_list_disc, norm_layer_list_disc, u_layer_list_disc, fiu_layer_list_disc, glogpfu_layer_list_disc))\n",
    "                    # size of output:([1, batch_size], [1, batch_size, batch_size])\n",
    "                    # the second loss term\n",
    "                    trace_layer_tensor_disc = torch.stack(trace_layer_list_disc, dim=0)\n",
    "                    trace_loss_disc = trace_layer_tensor_disc.sum(0)  \n",
    "                    # the third loss term\n",
    "                    norm_layer_tensor_disc = torch.stack(norm_layer_list_disc, dim=0)\n",
    "                    norm_loss_disc = norm_layer_tensor_disc.sum(0)\n",
    "                    if print_norm:\n",
    "                        print('DISC Norm of f_u:', norm_loss_disc.item())\n",
    "                    norm_loss_disc = lamda * norm_loss_disc # need to negativate this loss \n",
    "                    # the first term in the first loss term \n",
    "                    glogpfu_tensor_disc = torch.stack(glogpfu_layer_list_disc, dim=0)\n",
    "                    glogpu_loss_disc = glogpfu_tensor_disc.sum(0)\n",
    "                    glogpu_loss_disc = -1 * glogpu_loss_disc\n",
    "                    # the second term in the first loss term\n",
    "                    # call the API instead of setting our own function\n",
    "                    begin = time.time()                        \n",
    "                    glogpyffu_loss_disc = lamda_like * model.classification_likelihood_loss(y_batch_disc, \n",
    "                                            output_batch_disc, u_layer_list_disc, fiu_layer_list_disc, \n",
    "                                                glogpfu_layer_list_disc)                            \n",
    "                    end = time.time()\n",
    "                    if print_time:\n",
    "                        print('Compute likelihood time:', str(end - begin), 's')\n",
    "                    # the first loss term\n",
    "                    score_loss_disc = glogpu_loss_disc + glogpyffu_loss_disc\n",
    "                    # change here to form our own loss\n",
    "                    original_loss_disc = torch.abs(score_loss_disc + trace_loss_disc)\n",
    "                    loss_disc = original_loss_disc - norm_loss_disc\n",
    "                    if print_loss:\n",
    "                        print('DISC prior loss:', glogpu_loss_disc.item())\n",
    "                        print('DISC likelihood loss:', glogpyffu_loss_disc.item())\n",
    "                        print('DISC trace loss:', trace_loss_disc.item())\n",
    "                        print('DISC original loss:', original_loss_disc.item())\n",
    "                        print('DISC norm loss:', norm_loss_disc.item())\n",
    "                        print('DISC total loss:', loss_disc.item())\n",
    "\n",
    "                    torch.autograd.backward(-loss_disc) # nevigate it to max this loss\n",
    "                    optimizer.step()\n",
    "                    minibatch_iter_disc.set_postfix(loss=loss_disc.item())\n",
    "\n",
    "                    mean_batch_disc, covar_batch_disc = output_batch_disc\n",
    "\n",
    "                    f_batch_list_disc = []\n",
    "                    for t in range(0, task_dim):\n",
    "                        f_line_disc = torch.distributions.MultivariateNormal(loc=mean_batch_disc[t, :], \n",
    "                                                                              covariance_matrix=covar_batch_disc[t, :, :]).rsample(\n",
    "                            torch.Size([])) # sample f from q(f)\n",
    "                        f_batch_list_disc.append(f_line_disc)\n",
    "                    f_batch_disc = torch.stack(f_batch_list_disc, dim=1) # size: [batch_size, task_dim]\n",
    "                    '''\n",
    "                    f_batch_disc = torch.distributions.MultivariateNormal(loc=mean_batch_disc, \n",
    "                                                                          covariance_matrix=covar_batch_disc).rsample(torch.Size([]))\n",
    "                    f_batch_disc = f_batch_disc.squeeze(0) # size: [batch_size]\n",
    "                    '''\n",
    "                    y_predict_disc = model.classification_likelihood(f_batch_disc)\n",
    "                    correct_disc = (y_predict_disc == y_batch_disc).sum().item()\n",
    "                    total_disc = y_batch_disc.size(0)\n",
    "                    acc_disc = 100 * correct_disc / total_disc\n",
    "                    if print_metric:\n",
    "                        print(f\"MINI-BATCH DISC_ACC: {acc_disc}%\")\n",
    "                    correct_iter_disc += correct_disc\n",
    "                    total_iter_disc += total_disc\n",
    "\n",
    "            acc_iter_disc = 100 * correct_iter_disc / total_iter_disc\n",
    "            if print_metric:\n",
    "                print(f\"ITER DISC_ACC: {acc_iter_disc}%\")\n",
    "            correct_epoch_disc += correct_iter_disc\n",
    "            total_epoch_disc += total_iter_disc\n",
    "\n",
    "        acc_epoch_disc = 100 * correct_epoch_disc / total_epoch_disc\n",
    "        if print_metric:\n",
    "            print(f\"EPOCH DISC_ACC: {acc_epoch_disc}%\")\n",
    "\n",
    "        if print_loss:\n",
    "            print('-----------------------------------')\n",
    "\n",
    "    if num_gen != 0:\n",
    "        correct_epoch_gen = 0 # use to calculate correct prediction in one epoch\n",
    "        total_epoch_gen = 0 # use to calculate total prediction in one epoch\n",
    "        # fix hyperparameter v and generator G, update discriminator D num_nc times\n",
    "        gen_iter = tqdm.notebook.tqdm(range(num_gen), desc='Train Generator', leave=False)\n",
    "        for j in gen_iter:\n",
    "            # Within each iteration, we will go over each minibatch of data\n",
    "            minibatch_iter_gen = tqdm.notebook.tqdm(train_loader1, desc=\"Minibatch-Gen\", leave=False)\n",
    "            correct_iter_gen = 0 # use to calculate correct prediction in one iteration\n",
    "            total_iter_gen = 0 # use to calculate total prediction in one iteration\n",
    "            # fix discriminator D and train hyperparameter v with generator G\n",
    "            for x_batch_gen, y_batch_gen in minibatch_iter_gen:\n",
    "                x_batch_gen, y_batch_gen = x_batch_gen.to(device), y_batch_gen.to(device)\n",
    "                with num_likelihood_samples(num_samples):\n",
    "                    optimizer_gen.zero_grad()\n",
    "                    trace_layer_list_gen = []\n",
    "                    norm_layer_list_gen = []\n",
    "                    u_layer_list_gen = []\n",
    "                    fiu_layer_list_gen = []\n",
    "                    glogpfu_layer_list_gen = []\n",
    "                    x_batch_gen = model.feature_extractor.forward(x_batch_gen)\n",
    "                    output_batch_gen, trace_layer_list_gen, norm_layer_list_gen, u_layer_list_gen, fiu_layer_list_gen, glogpfu_layer_list_gen = model.forward(\n",
    "                                                                        (x_batch_gen, True, False, False, trace_layer_list_gen, \n",
    "                                                                        norm_layer_list_gen, u_layer_list_gen, fiu_layer_list_gen, \n",
    "                                                                        glogpfu_layer_list_gen))\n",
    "                    trace_layer_tensor_gen = torch.stack(trace_layer_list_gen, dim=0)\n",
    "                    trace_loss_gen = trace_layer_tensor_gen.sum(0)\n",
    "\n",
    "                    norm_layer_tensor_gen = torch.stack(norm_layer_list_gen, dim=0)\n",
    "                    norm_loss_gen = norm_layer_tensor_gen.sum(0)\n",
    "                    norm_loss_gen = lamda * norm_loss_gen # need to negativate this loss\n",
    "\n",
    "                    glogpfu_tensor_gen = torch.stack(glogpfu_layer_list_gen, dim=0)\n",
    "                    glogpu_loss_gen = glogpfu_tensor_gen.sum(0)\n",
    "                    glogpu_loss_gen = -1 * glogpu_loss_gen  # occur wrong when update generator G\n",
    "\n",
    "                    # the second term in the first loss term\n",
    "                    # call the API instead of setting our own function\n",
    "                    begin = time.time()\n",
    "                    glogpyffu_loss_gen = lamda_like * model.classification_likelihood_loss(y_batch_gen, \n",
    "                                            output_batch_gen, u_layer_list_gen, fiu_layer_list_gen, \n",
    "                                                glogpfu_layer_list_gen)\n",
    "                    end = time.time()\n",
    "                    if print_time:\n",
    "                        print('Compute likelihood time:', str(end - begin), 's')\n",
    "                    # the first loss term\n",
    "                    score_loss_gen = glogpu_loss_gen + glogpyffu_loss_gen\n",
    "                    # change here to form our own loss\n",
    "                    original_loss_gen = torch.abs(score_loss_gen + trace_loss_gen)\n",
    "                    loss_gen = original_loss_gen - norm_loss_gen\n",
    "\n",
    "                    if print_loss:\n",
    "                        print('GEN prior loss:', glogpu_loss_gen.item())\n",
    "                        print('GEN likelihood loss:', glogpyffu_loss_gen.item())\n",
    "                        print('GEN trace loss:', trace_loss_gen.item())\n",
    "                        print('GEN original loss:', original_loss_gen.item())\n",
    "                        print('GEN norm loss:', norm_loss_gen.item())\n",
    "                        print('GEN total loss:', loss_gen.item())\n",
    "\n",
    "                    torch.autograd.backward(loss_gen)\n",
    "                    optimizer_gen.step()\n",
    "                    minibatch_iter_gen.set_postfix(loss=loss_gen.item())\n",
    "\n",
    "                    mean_batch_gen, covar_batch_gen = output_batch_gen\n",
    "\n",
    "                    f_batch_list_gen = []\n",
    "                    for t in range(0, task_dim):\n",
    "                        f_line_gen = torch.distributions.MultivariateNormal(loc=mean_batch_gen[t, :], \n",
    "                                                                              covariance_matrix=covar_batch_gen[t, :, :]).rsample(\n",
    "                            torch.Size([])) # sample f from q(f)\n",
    "                        f_batch_list_gen.append(f_line_gen)\n",
    "                    f_batch_gen = torch.stack(f_batch_list_gen, dim=1)\n",
    "                    \n",
    "                    y_predict_gen = model.classification_likelihood(f_batch_gen)\n",
    "                    correct_gen = (y_predict_gen == y_batch_gen).sum().item()\n",
    "                    total_gen = y_batch_gen.size(0)\n",
    "                    acc_gen = 100 * correct_gen / total_gen\n",
    "                    if print_metric:\n",
    "                        print(f\"MINI-BATCH GEN_ACC: {acc_gen}%\")\n",
    "                    correct_iter_gen += correct_gen\n",
    "                    total_iter_gen += total_gen\n",
    "\n",
    "            acc_iter_gen = 100 * correct_iter_gen / total_iter_gen\n",
    "            if print_metric:\n",
    "                print(f\"ITER GEN_ACC: {acc_iter_gen}%\")\n",
    "            correct_epoch_gen += correct_iter_gen\n",
    "            total_epoch_gen += total_iter_gen\n",
    "\n",
    "        acc_epoch_gen = 100 * correct_epoch_gen / total_epoch_gen\n",
    "        if print_metric:\n",
    "            print(f\"EPOCH GEN_ACC: {acc_epoch_gen}%\")\n",
    "\n",
    "        if print_loss:\n",
    "            print('-----------------------------------')\n",
    "\n",
    "    if num_hyp != 0:\n",
    "        correct_epoch_hyp = 0 # use to calculate correct prediction in one epoch\n",
    "        total_epoch_hyp = 0 # use to calculate total prediction in one epoch\n",
    "        # fix hyperparameter v and generator G, update discriminator D num_nc times\n",
    "        hyp_iter = tqdm.notebook.tqdm(range(num_hyp), desc='Train Hyper-parameter', leave=False)\n",
    "        for j in hyp_iter:\n",
    "            # Within each iteration, we will go over each minibatch of data\n",
    "            minibatch_iter_hyp = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch-Hyper\", leave=False)\n",
    "            correct_iter_hyp = 0 # use to calculate correct prediction in one iteration\n",
    "            total_iter_hyp = 0 # use to calculate total prediction in one iteration\n",
    "            # fix discriminator D and train hyperparameter v with generator G\n",
    "            for x_batch_hyp, y_batch_hyp in minibatch_iter_hyp:\n",
    "                x_batch_hyp, y_batch_hyp = x_batch_hyp.to(device), y_batch_hyp.to(device)\n",
    "                with num_likelihood_samples(num_samples):\n",
    "                    optimizer1.zero_grad()\n",
    "                    x_batch_hyp = model.feature_extractor.forward(x_batch_hyp)\n",
    "                   \n",
    "                    \n",
    "                    output_batch_hyp, _, _, _, _, _ = model.forward((x_batch_hyp, False, False, True, [], [], [], [], []))\n",
    "                    # call the API instead of setting our own function\n",
    "                    mean_batch_hyp, covar_batch_hyp = output_batch_hyp\n",
    "\n",
    "                    f_batch_list_hyp = []\n",
    "                    for t in range(0, task_dim):\n",
    "                        f_line_hyp = torch.distributions.MultivariateNormal(loc=mean_batch_hyp[t, :], \n",
    "                                                                              covariance_matrix=covar_batch_hyp[t, :, :]).rsample(\n",
    "                            torch.Size([])) # sample f from q(f)\n",
    "                        f_batch_list_hyp.append(f_line_hyp)\n",
    "                    f_batch_hyp = torch.stack(f_batch_list_hyp, dim=1)\n",
    "                    \n",
    "                    \n",
    "                    model.feature_extractor.requires_grad_(True)\n",
    "                    loss_hyp = model.cross(f_batch_hyp, y_batch_hyp)\n",
    "\n",
    "                    if print_loss:\n",
    "                            print('HYP total loss:', loss_hyp.item())\n",
    "                    torch.autograd.backward(loss_hyp)\n",
    "                    optimizer1.step()\n",
    "                    minibatch_iter_hyp.set_postfix(loss=loss_hyp.item())\n",
    "\n",
    "                    y_predict_hyp = model.classification_likelihood(f_batch_hyp)\n",
    "               \n",
    "                    correct_hyp = (y_predict_hyp == y_batch_hyp).sum().item()\n",
    "                    total_hyp = y_batch_hyp.size(0)\n",
    "                    acc_hyp = 100 * correct_hyp / total_hyp\n",
    "                    if print_metric:\n",
    "                        print(f\"MINI-BATCH HYP_ACC: {acc_hyp}%\")\n",
    "                    correct_iter_hyp += correct_hyp\n",
    "                    total_iter_hyp += total_hyp\n",
    "\n",
    "            acc_iter_hyp = 100 * correct_iter_hyp / total_iter_hyp\n",
    "            if print_metric:\n",
    "                print(f\"ITER HYP_ACC: {acc_iter_hyp}%\")\n",
    "            correct_epoch_hyp += correct_iter_hyp\n",
    "            total_epoch_hyp += total_iter_hyp\n",
    "            \n",
    "            # store test rmse each iteration\n",
    "            acc_test_hyp= model.predict(test_loader)\n",
    "\n",
    "            acc_test_list.append(acc_test_hyp)\n",
    "            # store test rmse each iteration\n",
    "            acc_train_list.append(acc_iter_hyp)\n",
    "\n",
    "\n",
    "        acc_epoch_hyp = 100 * correct_epoch_hyp / total_epoch_hyp\n",
    "        if print_metric:\n",
    "            print(f\"EPOCH HYP_ACC: {acc_epoch_hyp}%\")\n",
    "\n",
    "        if print_loss:\n",
    "            print('-----------------------------------')\n",
    "end_sum = time.time()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import math\n",
    "\n",
    "# evaluate stage\n",
    "model.eval()\n",
    "\n",
    "\n",
    "total_acc = model.predict(test_loader)\n",
    "print('Total Accuracy: {}%'.format(total_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xujian",
   "language": "python",
   "name": "xujian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

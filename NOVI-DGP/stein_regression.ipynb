{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.variational import VariationalStrategy\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:4' # change here to tune the device we use\n",
    "else:\n",
    "    device = 'cpu'\n",
    "# device = 'cpu'\n",
    "\n",
    "# load data and pre-processing\n",
    "# regression: UCI dataset, classification: Image dataset\n",
    "dataset = 'energy' # set here to determine to train which dataset\n",
    "if dataset == 'elevators':\n",
    "    # D=18, N=13768\n",
    "    data = torch.Tensor(loadmat('data/elevators.mat')['data'])\n",
    "    X = data[:, :-1]  # the last column is label\n",
    "    y = data[:, -1]\n",
    "elif dataset == 'energy':\n",
    "    # D=8, N=768\n",
    "    data = pd.read_excel('data/energy.xlsx').values\n",
    "    X = torch.Tensor(data[:, :8]) # the last two column is label\n",
    "    y1 = torch.Tensor(data[:, 8])\n",
    "    y2 = torch.Tensor(data[:, 9])\n",
    "    y = y1 # change here to run on another label\n",
    "elif dataset == 'protein':\n",
    "    # D=9, N=45730\n",
    "    data = pd.read_csv('data/protein.csv').values\n",
    "    X = torch.Tensor(data[:, 1:])\n",
    "    y = torch.Tensor(data[:, 0]) # the first column is label\n",
    "elif dataset == 'power':\n",
    "    # D=4, N=9568\n",
    "    sheet_num = 0 # power data has five sheet, change it from 0 to 4\n",
    "    data = pd.read_excel('data/power.xlsx', sheet_name=sheet_num).values\n",
    "    X = torch.Tensor(data[:, :4])\n",
    "    y = torch.Tensor(data[:, 4])\n",
    "elif dataset == 'concrete':\n",
    "    # D=8, N=1030\n",
    "    data = pd.read_excel('data/concrete.xls', header=0).values\n",
    "    X = torch.Tensor(data[:, :8])\n",
    "    y = torch.Tensor(data[:, 8])\n",
    "elif dataset == 'year_msd':\n",
    "    # D=90, N=515345\n",
    "    # data = np.loadtxt('data/year_msd.txt', delimiter=',')\n",
    "    raise NotImplementedError('It has not been implemented yet')\n",
    "    data = pd.read_csv('data/year_msd.txt', header=None, delimiter=',').values\n",
    "    X = torch.Tensor(data[:, 1:])\n",
    "    y = torch.Tensor(data[:, 0]) # the first column is label\n",
    "elif dataset == 'boston':\n",
    "    # D=13, N=506\n",
    "    from sklearn.datasets import load_boston    \n",
    "    boston = load_boston()\n",
    "    X = torch.Tensor(boston.data)\n",
    "    y = torch.Tensor(boston.target)\n",
    "elif dataset == 'kin8nm':\n",
    "    # D=8, N=8192\n",
    "    data = pd.read_csv('data/kin8nm.csv', header=None).values\n",
    "    X = torch.Tensor(data[:, :8])\n",
    "    y = torch.Tensor(data[:, 8])\n",
    "elif dataset == 'yacht':\n",
    "    # D=6, N=308\n",
    "    data = pd.read_csv('data/yacht.csv', header=None).values\n",
    "    X = torch.Tensor(data[:, :6])\n",
    "    y = torch.Tensor(data[:, 6])\n",
    "elif dataset == 'qsar':\n",
    "    # D=8, N=546\n",
    "    data = pd.read_csv('data/qsar.csv', header=None).values\n",
    "    X = torch.Tensor(data[:, :8])\n",
    "    y = torch.Tensor(data[:, 8])\n",
    "    \n",
    "# pre-processing\n",
    "X = X - X.min(0)[0]  # X.min(0)[0]: min value of every feature\n",
    "X = 2 * (X / X.max(0)[0]) - 1 # pre-preocess to [-1,1]\n",
    "# X -= X.mean(0)\n",
    "# X /= X.std(0)\n",
    "y -= y.mean()\n",
    "y /= y.std() # pre-process to N(0,1)\n",
    "\n",
    "# split data\n",
    "ratio = 0.9 # ratio to split train set\n",
    "train_n = int(floor(ratio * len(X))) # split train set, ratio: 0.8\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "        \n",
    "# move data to cuda or cpu\n",
    "train_x, train_y, test_x, test_y = train_x.to(device), train_y.to(device), test_x.to(device), test_y.to(device)\n",
    "\n",
    "def get_parameter_number(model):\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "if dataset == 'elevators':\n",
    "    loader_batch_size = 1024\n",
    "elif dataset == 'energy':\n",
    "    loader_batch_size = 64\n",
    "elif dataset == 'protein':\n",
    "    loader_batch_size = 2048\n",
    "elif dataset == 'power':\n",
    "    loader_batch_size = 1024\n",
    "elif dataset == 'concrete':\n",
    "    loader_batch_size = 128\n",
    "    loader_batch_size = 512\n",
    "elif dataset == 'year_msd':\n",
    "    loader_batch_size = 8192\n",
    "elif dataset == 'boston':\n",
    "    loader_batch_size = 64\n",
    "elif dataset == 'kin8nm':\n",
    "    loader_batch_size = 1024\n",
    "elif dataset == 'yacht':\n",
    "    loader_batch_size = 128\n",
    "elif dataset == 'qsar':\n",
    "    loader_batch_size = 128\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=loader_batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=loader_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_time = False # set True to print module time\n",
    "print_norm = False # set True to print norm of f_u (the perfect value of norm after training: 0)\n",
    "print_metric = True # set True to print metric per epoch\n",
    "print_param = True # set True to print parameter size of each network\n",
    "print_loss = False # set True to print each loss during training\n",
    "sample_times = 10 # Sample k times to evaluate the expectation over q(u)\n",
    "concat_type = False # set True to concat e with z in generator G\n",
    "expect_mean = False # Set True to compute mean of q(f) by average over q(u)\n",
    "\n",
    "class ToyDeepGPHiddenLayer(DeepGPLayer):  # input_dims: size of feature dim\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=128, mean_type='constant', stein=False, noise_add=True,\n",
    "                 noise_share=False, noise_dim=1, multi_head=False, vector_type=False):\n",
    "        # TODO: adjust the size of inducing_points to [inducing_size, input_dim] for each layer\n",
    "        if stein is False:\n",
    "            if output_dims is None:\n",
    "                inducing_points = torch.randn((num_inducing, input_dims), requires_grad=True) # sample from gaussian dist\n",
    "                batch_shape = torch.Size([])\n",
    "            else: # inducing_points-dim correspond to: output_dims, num_inducing and input_dims\n",
    "                inducing_points = torch.randn((output_dims, num_inducing, input_dims), requires_grad=True)\n",
    "                batch_shape = torch.Size([output_dims])\n",
    "        else:\n",
    "            if output_dims is None:\n",
    "                inducing_points = torch.randn((num_inducing, input_dims), requires_grad=True) # sample from gaussian dist\n",
    "                batch_shape = torch.Size([])\n",
    "            else: # inducing_points-dim correspond to: [num_inducing, input_dims]\n",
    "                inducing_points = torch.randn((num_inducing, input_dims), requires_grad=True)\n",
    "                batch_shape = torch.Size([output_dims])\n",
    "        \n",
    "        self.input_dim = input_dims\n",
    "        self.output_dim = output_dims\n",
    "        self.inducing_size = num_inducing\n",
    "\n",
    "        self.noise_add = noise_add\n",
    "        self.noise_share = noise_share\n",
    "        self.noise_dim = noise_dim # get this value from the outer model\n",
    "        if self.noise_dim is None:\n",
    "            self.noise_dim = 32\n",
    "        self.noise_add = noise_add\n",
    "        if self.noise_add:\n",
    "            self.concat_type = concat_type\n",
    "        else:\n",
    "            self.concat_type = False # only can set True when adding noise\n",
    "\n",
    "        # self.share_noise = share_noise # noise shared by all layers\n",
    "        self.multi_head = multi_head\n",
    "        # self.transformed_noise = transformed_noise\n",
    "        self.learn_inducing_locations = True # Set True to treat location of z as parameter\n",
    "        \n",
    "        self.hutch_times = 1 # number for hutchison estimation of trace\n",
    "        self.bottleneck_trick = True # Set True to introduce decomposition of jacobian to reduce variance\n",
    "        self.rademacher_type = False # Set True to sample from Rademecher dist to compute trace\n",
    "        self.diagonal_type = True # Set True to use diagonal kernel matrix to approximate fully kernel matrix\n",
    "        self.vector_type = vector_type # Set True to generate u using vector-based network\n",
    "        \n",
    "        self.print_time = print_time\n",
    "        self.print_param = print_param\n",
    "        self.device = device\n",
    "        self.sample_times = sample_times # Sample k times to evaluate the expectation over q(u)\n",
    "        self.expect_mean = expect_mean\n",
    "        \n",
    "        variational_distribution = None\n",
    "\n",
    "        variational_strategy = VariationalStrategy( # transform q(U) to q(F)\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            stein_type=stein,\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "\n",
    "        super(ToyDeepGPHiddenLayer, self).__init__(variational_strategy, input_dims, output_dims)        \n",
    "        self.variational_strategy = variational_strategy\n",
    "\n",
    "    def forward(self, x):\n",
    "        return None\n",
    "    \n",
    "    # will be called when run the forward function of class DeepGP\n",
    "    def __call__(self, x, train_type=True, disc_type=True, hyp_type=True, trace_layer_list=None, norm_layer_list=None, u_layer_list=None,\n",
    "                 fiu_layer_list=None, glogpfu_layer_list=None, shared_list=None, transformed_list=None, *other_inputs,\n",
    "                 **kwargs): \n",
    "        \"\"\"\n",
    "        Overriding __call__ isn't strictly necessary, but it lets us add concatenation based skip connections\n",
    "        easily.\n",
    "        \"\"\"\n",
    "        return super().__call__(x, are_samples=False, train_type=train_type, disc_type=disc_type,\n",
    "                                hyp_type=hyp_type, trace_layer_list=trace_layer_list, norm_layer_list=norm_layer_list,\n",
    "                                u_layer_list=u_layer_list, fiu_layer_list=fiu_layer_list, \n",
    "                                glogpfu_layer_list=glogpfu_layer_list, shared_list=shared_list,\n",
    "                               transformed_list=transformed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_dims = 10  # number of output_dim in hidden layer\n",
    "vector_type = True # Set True to generate u using vector-based network\n",
    "learn_likelihood_covariance = True # Set True to treat the covariance of likelihood as parameter\n",
    "task_dim = None\n",
    "train_x_shape = train_x.shape[-1]\n",
    "        \n",
    "# we can fine-tune it to obtain a better result\n",
    "if dataset == 'elevators':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'energy':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'protein':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'power':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'concrete':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'year_msd':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'boston':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'kin8nm':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'yacht':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'qsar':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "    \n",
    "    \n",
    "class DeepGP(DeepGP): # define the noise outside the layer\n",
    "    def __init__(self, train_x_shape, stein=False, noise_add=True, noise_share=False, multi_head=False): # L=2\n",
    "        if noise_add is False and noise_share:\n",
    "            raise ValueError('Only can share noise across layer when noise is added')\n",
    "        if noise_share is False and multi_head:\n",
    "            raise ValueError('Only can use Multi-head Mechanism when noise is shared across layer')\n",
    "        super().__init__()\n",
    "        self.vector_type = vector_type\n",
    "        self.noise_add = noise_add\n",
    "        self.noise_share = noise_share # share the noise across layer or not\n",
    "        # fine-tune this term or directly learn as a prior\n",
    "        self.noise_dim = noise_dim\n",
    "        self.multi_head = multi_head  # transform noise or not        \n",
    "        self.back_bone = nn.Sequential(\n",
    "            nn.Linear(in_features=self.noise_dim, out_features=32),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=32, out_features=self.noise_dim)\n",
    "        )\n",
    "        if self.multi_head and print_param:\n",
    "            print('Generator backbone:', get_parameter_number(self.back_bone))\n",
    "        \n",
    "        lls_sigma = torch.Tensor([1.0]) # Set sigma to be a small number\n",
    "        if learn_likelihood_covariance:\n",
    "            self.register_parameter(\"lls_sigma\", torch.nn.Parameter(lls_sigma))\n",
    "        else:\n",
    "            self.register_buffer(\"lls_sigma\", lls_sigma)\n",
    "        \n",
    "        hidden_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=train_x_shape,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "        \n",
    "        hidden_layer2 = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer.output_dim,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )   \n",
    "        \n",
    "        '''\n",
    "        hidden_layer3 = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer2.output_dim,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "        '''\n",
    "        '''\n",
    "        hidden_layer4 = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer3.output_dim,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "        '''\n",
    "\n",
    "        last_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer2.output_dim,\n",
    "            output_dims=None,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='constant',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.hidden_layer2 = hidden_layer2\n",
    "        # self.hidden_layer3 = hidden_layer3\n",
    "        # self.hidden_layer4 = hidden_layer4\n",
    "        self.last_layer = last_layer\n",
    "        \n",
    "        # register all networks here!!!\n",
    "        self.hidden_strategy_generator = self.hidden_layer.variational_strategy.generator\n",
    "        self.hidden_strategy_discriminator = self.hidden_layer.variational_strategy.discriminator\n",
    "        self.hidden_kernel_method = self.hidden_layer.variational_strategy.kernel_method\n",
    "        self.hidden2_strategy_generator = self.hidden_layer2.variational_strategy.generator\n",
    "        self.hidden2_strategy_discriminator = self.hidden_layer2.variational_strategy.discriminator\n",
    "        self.hidden2_kernel_method = self.hidden_layer2.variational_strategy.kernel_method\n",
    "        '''\n",
    "        self.hidden3_strategy_generator = self.hidden_layer3.variational_strategy.generator\n",
    "        self.hidden3_strategy_discriminator = self.hidden_layer3.variational_strategy.discriminator\n",
    "        self.hidden3_kernel_method = self.hidden_layer3.variational_strategy.kernel_method\n",
    "        \n",
    "        self.hidden4_strategy_generator = self.hidden_layer4.variational_strategy.generator\n",
    "        self.hidden4_strategy_discriminator = self.hidden_layer4.variational_strategy.discriminator\n",
    "        self.hidden4_kernel_method = self.hidden_layer4.variational_strategy.kernel_method\n",
    "        '''\n",
    "        self.last_strategy_generator = self.last_layer.variational_strategy.generator\n",
    "        self.last_strategy_discriminator = self.last_layer.variational_strategy.discriminator\n",
    "        self.last_kernel_method = self.last_layer.variational_strategy.kernel_method\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # set train_type=False when evaluate, do not compute the loss to faster the process\n",
    "        # Generate noise here when noise is shared or transformed\n",
    "        if self.noise_add:\n",
    "            if self.noise_share:\n",
    "                if self.multi_head:\n",
    "                    if self.vector_type:\n",
    "                        shared_list = []\n",
    "                        transformed_list = []\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = torch.randn(self.noise_dim, requires_grad=True).to(device)\n",
    "                            transformed_noise = self.back_bone(shared_noise)\n",
    "                            shared_list.append(shared_noise)\n",
    "                            transformed_list.append(transformed_noise)\n",
    "                    else:\n",
    "                        shared_list = []\n",
    "                        transformed_list = []\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = torch.randn((num_layer_inducing, self.noise_dim), requires_grad=True).to(device)\n",
    "                            transformed_noise = self.back_bone(shared_noise)\n",
    "                            shared_list.append(shared_noise)\n",
    "                            transformed_list.append(transformed_noise)\n",
    "                else:\n",
    "                    if self.vector_type:\n",
    "                        shared_list = []\n",
    "                        transformed_list = None\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = torch.randn(self.noise_dim, requires_grad=True).to(device)\n",
    "                            shared_list.append(shared_noise)                        \n",
    "                    else:\n",
    "                        shared_list = []\n",
    "                        transformed_list = None\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = torch.randn((num_layer_inducing, self.noise_dim), requires_grad=True).to(device)\n",
    "                            shared_list.append(shared_noise)  \n",
    "            else:\n",
    "                shared_list = None\n",
    "                transformed_list = None\n",
    "        else:\n",
    "            shared_list = None\n",
    "            transformed_list = None\n",
    "        inputs, train_type, disc_type, hyp_type, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = inputs\n",
    "        # determine to train backbone network and parameter sigma or not\n",
    "        if train_type:\n",
    "            if disc_type:\n",
    "                self.back_bone.requires_grad_(False)\n",
    "                self.lls_sigma.requires_grad_(False)\n",
    "            else:\n",
    "                self.back_bone.requires_grad_(True)\n",
    "                self.lls_sigma.requires_grad_(False)\n",
    "        else:\n",
    "            if hyp_type:\n",
    "                self.back_bone.requires_grad_(False)\n",
    "                self.lls_sigma.requires_grad_(True)\n",
    "            else: # use for test stage\n",
    "                self.back_bone.requires_grad_(False)\n",
    "                self.lls_sigma.requires_grad_(False)\n",
    "        \n",
    "        hidden_rep1, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer(\n",
    "                                                                            inputs, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        \n",
    "        hidden_rep2, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer2(\n",
    "                                                                            hidden_rep1, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        '''\n",
    "        hidden_rep3, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer2(\n",
    "                                                                            hidden_rep2, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        \n",
    "        hidden_rep4, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer2(\n",
    "                                                                            hidden_rep3, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        '''\n",
    "        \n",
    "        # get a prob distribution, not a deterministic vector value\n",
    "        output = self.last_layer(hidden_rep2, train_type=train_type, disc_type=disc_type, hyp_type=hyp_type, \n",
    "                                 trace_layer_list=trace_layer_list, norm_layer_list=norm_layer_list, \n",
    "                                 u_layer_list=u_layer_list, fiu_layer_list=fiu_layer_list, \n",
    "                                 glogpfu_layer_list=glogpfu_layer_list, shared_list=shared_list, \n",
    "                                 transformed_list=transformed_list)\n",
    "        return output\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        mus = []\n",
    "        lls = []\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            output_batch, _, _, _, _, _ = self.forward((x_batch, False, False, False, [], [], [], [], []))\n",
    "            mean_batch, covar_batch = output_batch\n",
    "            f_batch = torch.distributions.MultivariateNormal(loc=mean_batch, covariance_matrix=covar_batch).rsample(\n",
    "                    torch.Size([])) # sample f from q(f)\n",
    "            f_batch = f_batch.squeeze(0) # size: [batch_size]\n",
    "            preds = self.regression_likelihood(f_batch) # change here when dealing with classification\n",
    "            mus.append(mean_batch.squeeze(0))\n",
    "            lls.append(self.test_regression_lls(preds, y_batch))\n",
    "                        \n",
    "        return torch.cat(mus, dim=0), torch.cat(lls, dim=0)\n",
    "\n",
    "    \n",
    "    def regression_likelihood_loss(self, y_batch, output_batch, u_layer_list, fiu_layer_list, glogpfu_layer_list):\n",
    "        # compute the likelihood loss without using the gpytorch framework\n",
    "        # We now assume that output_batch == (mean_batch, covariance_batch)\n",
    "        # We assume that it is single-task and final output_dim == 1\n",
    "        # We now set S=1, so we just sample from q(f) once\n",
    "        mean_batch, covariance_batch = output_batch\n",
    "        f_batch = torch.distributions.MultivariateNormal(loc=mean_batch, covariance_matrix=covariance_batch).rsample(torch.Size([]))\n",
    "        f_batch = torch.reshape(f_batch, (y_batch.shape[0],))\n",
    "        distance_batch = y_batch - f_batch\n",
    "        \n",
    "        glogpyf_layer_list = [] # use to contain gradient of likelihood\n",
    "        for l in range(0, len(u_layer_list)):\n",
    "            glogpyf_l_list = []\n",
    "            for k in range(0, sample_times):\n",
    "                if expect_mean:\n",
    "                    glogpyf_l = torch.autograd.grad(f_batch, u_layer_list[l][k], grad_outputs=distance_batch, \n",
    "                                                    retain_graph=True, create_graph=True)[0]\n",
    "                else:\n",
    "                    # Set expect_mean == False to generate q(f) only using the first element of u_layer_list\n",
    "                    glogpyf_l = torch.autograd.grad(f_batch, u_layer_list[l][0], grad_outputs=distance_batch, \n",
    "                                                    retain_graph=True, create_graph=True)[0]\n",
    "                glogpyf_l_list.append(glogpyf_l)\n",
    "            glogpyf_layer_list.append(glogpyf_l_list)\n",
    "        \n",
    "        glogpyffu_layer_list = []\n",
    "        # glogpyffu_layer_list: the second term in the first loss term in each layer\n",
    "        for l in range(0, len(u_layer_list)):\n",
    "            glogpyffu_l_list = []\n",
    "            for k in range(0, sample_times):\n",
    "                glogpyffu_l = torch.mm(glogpyf_layer_list[l][k].unsqueeze(0), fiu_layer_list[l][k].unsqueeze(1))\n",
    "                glogpyffu_l_list.append(glogpyffu_l)\n",
    "            glogpyffu_l_tensor = torch.stack(glogpyffu_l_list, dim=0)\n",
    "            glogpyffu_l_mean = glogpyffu_l_tensor.mean(0) # average over sample_times\n",
    "            glogpyffu_layer_list.append(glogpyffu_l_mean)\n",
    "        glogpyffu_tensor = torch.stack(glogpyffu_layer_list, dim=0)\n",
    "        glogpyffu_loss = glogpyffu_tensor.sum(0)\n",
    "        \n",
    "        sigma = self.lls_sigma.to(device)\n",
    "        # sigma = torch.Tensor([1.0]).to(device)\n",
    "        sigma2 = torch.pow(sigma, 2)\n",
    "        glogpyffu_loss = torch.div(glogpyffu_loss, sigma2)\n",
    "        \n",
    "        return glogpyffu_loss\n",
    "        \n",
    "    # TODO: Still need to fix it \n",
    "    def test_regression_lls(self, y_predict, y_label): # use for regression\n",
    "        # return the LL vector, size: [batch_size]\n",
    "        # We now assume dim == 1\n",
    "        # dim = y_label.shape[0]\n",
    "        # size of y_label: [batch_size]\n",
    "        sigma = self.lls_sigma.to(device)\n",
    "        # sigma = torch.Tensor([1.0]).to(device)\n",
    "        first_term = torch.log(torch.Tensor([2 * math.pi]))\n",
    "        first_term = torch.mul(first_term, -0.5).to(device)\n",
    "        second_term = torch.log(sigma)\n",
    "        second_term = torch.mul(second_term, -1)\n",
    "        res = y_label - y_predict\n",
    "        res2 = torch.mul(res, res)\n",
    "        # third_term = torch.mm(res.unsqueeze(0), res.unsqueeze(1))\n",
    "        sigma2 = torch.pow(sigma, 2)\n",
    "        third_factor_term = torch.div(torch.Tensor([-1]).to(device), torch.mul(sigma2, 2))\n",
    "        third_term = torch.mul(res2, third_factor_term)\n",
    "        # third_term = third_vector.mean(0) # average batch_size\n",
    "        # test_ll = first_term + second_term + third_term\n",
    "        test_ll = third_term.add(first_term).add(second_term)\n",
    "        return test_ll\n",
    "    \n",
    "    def regression_likelihood(self, f_batch):\n",
    "        # We assume that size of f_batch: [batch_size]\n",
    "        # So we return y_predict with size: [batch_size]\n",
    "        # We add a gaussian noise to f to form the final predict value y\n",
    "        dim = f_batch.shape[0]\n",
    "        sigma_batch = self.lls_sigma.to(device)\n",
    "        noise_batch = torch.randn(dim).to(device)\n",
    "        noise_final_batch = torch.mul(noise_batch, sigma_batch)\n",
    "        y_predict = f_batch + noise_final_batch\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator backbone: {'Total': 2112, 'Trainable': 2112}\n",
      "Generator: {'Total': 44320, 'Trainable': 44320}\n",
      "Discriminator: {'Total': 42256, 'Trainable': 42256}\n",
      "Kernel method: {'Total': 10, 'Trainable': 10}\n",
      "Generator: {'Total': 44576, 'Trainable': 44576}\n",
      "Discriminator: {'Total': 42256, 'Trainable': 42256}\n",
      "Kernel method: {'Total': 12, 'Trainable': 12}\n",
      "Generator: {'Total': 6560, 'Trainable': 6560}\n",
      "Discriminator: {'Total': 4240, 'Trainable': 4240}\n",
      "Kernel method: {'Total': 12, 'Trainable': 12}\n",
      "Test whether you have cuda to run the process or not: True\n",
      "{'Total': 186355, 'Trainable': 186355}\n"
     ]
    }
   ],
   "source": [
    "# NOTE: When strange error occur (especially wrong error line), restart the kernel\n",
    "# If multi_head=True, Transform shared noise by a network and concat with layer-specified inducing points\n",
    "model = DeepGP(train_x_shape, stein=True, noise_add=True, noise_share=True, multi_head=True)\n",
    "print('Test whether you have cuda to run the process or not:', torch.cuda.is_available())\n",
    "model = model.to(device)\n",
    "print(get_parameter_number(model))\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.settings import num_likelihood_samples\n",
    "\n",
    "def train(num_epochs=3, num_disc=2, num_gen=2, num_hyp=10, num_samples=10, lamda=100, lamda_like=1):\n",
    "    '''\n",
    "    num_epochs = 3\n",
    "    num_disc = 2 # use to train discriminator per epoch\n",
    "    num_gen = 2 # use to train generator per epoch\n",
    "    num_hyp = 5 # use to train hyper-parameter per epoch\n",
    "    num_samples = 10 # use to evaluate likelihood\n",
    "    lamda = 100 # hyperparameter for norm loss\n",
    "    lamda_like = 1 # hyperparameter for likelihood loss (will be deleted soon)\n",
    "    '''\n",
    "    if num_epochs == 0:\n",
    "        raise ValueError('At least one epoch need to be run!!!')\n",
    "    \n",
    "    epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    for i in epochs_iter: # change here to add adversarial training\n",
    "        print('New epoch!!!')\n",
    "        if num_disc != 0:\n",
    "            rmse_iter_list_disc = [] # list used to contain average rmse in each iteration\n",
    "            lls_iter_list_disc = [] # list used to contain average log-likelihood in each iteration\n",
    "            # fix hyperparameter v and generator G, update discriminator D num_nc times\n",
    "            disc_iter = tqdm.notebook.tqdm(range(num_disc), desc='Train Discriminator', leave=False)\n",
    "            for j in disc_iter:\n",
    "                # Within each iteration, we will go over each minibatch of data\n",
    "                minibatch_iter_disc = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch-Disc\", leave=False)\n",
    "                rmse_batch_list_disc = [] # list used to store rmse in mini-batch\n",
    "                lls_batch_list_disc = [] # list used to store log-likelihood in mini-batch\n",
    "                for x_batch_disc, y_batch_disc in minibatch_iter_disc:\n",
    "                    x_batch_disc, y_batch_disc = x_batch_disc.to(device), y_batch_disc.to(device)\n",
    "                    with num_likelihood_samples(num_samples):\n",
    "                        optimizer.zero_grad()\n",
    "                        trace_layer_list_disc = []\n",
    "                        norm_layer_list_disc = []\n",
    "                        u_layer_list_disc = []\n",
    "                        fiu_layer_list_disc = []\n",
    "                        glogpfu_layer_list_disc = []\n",
    "                        # trace_layer_list: contain the final trace in each layer\n",
    "                        # norm_layer_list: contain the norm in each layer\n",
    "                        # u_layer_list: contain u in each layer\n",
    "                        # fiu_layer_list: contain f_u in each layer\n",
    "                        # glogpu_layer_list: contain dot product of gradient of logp_u with f_u in each layer\n",
    "                        # disc_type: determine update discriminator or generator\n",
    "                        output_batch_disc, trace_layer_list_disc, norm_layer_list_disc, u_layer_list_disc, fiu_layer_list_disc, glogpfu_layer_list_disc = model.forward(\n",
    "                            (x_batch_disc, True, True, False, trace_layer_list_disc, norm_layer_list_disc, u_layer_list_disc, fiu_layer_list_disc, glogpfu_layer_list_disc))\n",
    "                        # size of output:([1, batch_size], [1, batch_size, batch_size])\n",
    "                        # the second loss term\n",
    "                        trace_layer_tensor_disc = torch.stack(trace_layer_list_disc, dim=0)\n",
    "                        trace_loss_disc = trace_layer_tensor_disc.sum(0)  \n",
    "                        # the third loss term\n",
    "                        norm_layer_tensor_disc = torch.stack(norm_layer_list_disc, dim=0)\n",
    "                        norm_loss_disc = norm_layer_tensor_disc.sum(0)\n",
    "                        if print_norm:\n",
    "                            print('DISC Norm of f_u:', norm_loss_disc.item())\n",
    "                        norm_loss_disc = lamda * norm_loss_disc # need to negativate this loss \n",
    "                        # the first term in the first loss term \n",
    "                        glogpfu_tensor_disc = torch.stack(glogpfu_layer_list_disc, dim=0)\n",
    "                        glogpu_loss_disc = glogpfu_tensor_disc.sum(0)\n",
    "                        glogpu_loss_disc = -1 * glogpu_loss_disc\n",
    "                        # the second term in the first loss term\n",
    "                        # call the API instead of setting our own function\n",
    "                        begin = time.time()\n",
    "                        glogpyffu_loss_disc = lamda_like * model.regression_likelihood_loss(y_batch_disc, \n",
    "                                                output_batch_disc, u_layer_list_disc, fiu_layer_list_disc, \n",
    "                                                    glogpfu_layer_list_disc)                          \n",
    "                        end = time.time()\n",
    "                        if print_time:\n",
    "                            print('Compute likelihood time:', str(end - begin), 's')\n",
    "                        # the first loss term\n",
    "                        score_loss_disc = glogpu_loss_disc + glogpyffu_loss_disc\n",
    "                        # change here to form our own loss\n",
    "                        original_loss_disc = torch.abs(score_loss_disc + trace_loss_disc)\n",
    "                        loss_disc = original_loss_disc - norm_loss_disc\n",
    "                        if print_loss:\n",
    "                            print('DISC prior loss:', glogpu_loss_disc.item())\n",
    "                            print('DISC likelihood loss:', glogpyffu_loss_disc.item())\n",
    "                            print('DISC trace loss:', trace_loss_disc.item())\n",
    "                            print('DISC original loss:', original_loss_disc.item())\n",
    "                            print('DISC norm loss:', norm_loss_disc.item())\n",
    "                            print('DISC total loss:', loss_disc.item())\n",
    "\n",
    "                        torch.autograd.backward(-loss_disc) # nevigate it to max this loss\n",
    "                        optimizer.step()\n",
    "                        minibatch_iter_disc.set_postfix(loss=loss_disc.item())\n",
    "\n",
    "                        mean_batch_disc, covar_batch_disc = output_batch_disc\n",
    "                        '''\n",
    "                        f_batch_list_disc = []\n",
    "                        for t in range(0, covar_batch_disc.shape[0]):\n",
    "                            f_line_disc = torch.distributions.MultivariateNormal(loc=mean_batch_disc[i, :], \n",
    "                                                                                  covariance_matrix=covar_batch_disc[i, :, :]).rsample(\n",
    "                                torch.Size([])) # sample f from q(f)\n",
    "                            f_batch_list_disc.append(f_line_disc)\n",
    "                        f_batch_disc = torch.stack(f_batch_list_disc, dim=1)\n",
    "                        '''\n",
    "                        f_batch_disc = torch.distributions.MultivariateNormal(loc=mean_batch_disc, \n",
    "                                                                              covariance_matrix=covar_batch_disc).rsample(torch.Size([]))\n",
    "                        f_batch_disc = f_batch_disc.squeeze(0) # size: [batch_size]\n",
    "                        y_predict_disc = model.regression_likelihood(f_batch_disc)\n",
    "                        rmse_batch_disc = torch.mean(torch.pow(y_predict_disc - y_batch_disc, 2)).sqrt() # rmse value in mini-batch\n",
    "                        lls_batch_disc = model.test_regression_lls(y_predict_disc, y_batch_disc).mean(0)\n",
    "                        rmse_batch_list_disc.append(rmse_batch_disc)\n",
    "                        lls_batch_list_disc.append(lls_batch_disc)\n",
    "                            \n",
    "                            \n",
    "\n",
    "                rmse_iter_disc = torch.stack(rmse_batch_list_disc, dim=0).mean(0) # rmse in one iteration\n",
    "                lls_iter_disc = torch.stack(lls_batch_list_disc, dim=0).mean(0) # log-likelihood in one iteration\n",
    "                if print_metric:\n",
    "                    print(f\"ITER DISC_RMSE: {rmse_iter_disc.item()}, ITER DISC_NLL: {-lls_iter_disc.item()}\")\n",
    "                rmse_iter_list_disc.append(rmse_iter_disc)\n",
    "                lls_iter_list_disc.append(lls_iter_disc)\n",
    "            \n",
    "\n",
    "            rmse_epoch_disc = torch.stack(rmse_iter_list_disc, dim=0).mean(0) # average rmse in one epoch\n",
    "            lls_epoch_disc = torch.stack(lls_iter_list_disc, dim=0).mean(0) # average log-likelihood in one epoch\n",
    "            if print_metric:\n",
    "                print(f\"EPOCH DISC_RMSE: {rmse_epoch_disc.item()}, EPOCH DISC_NLL: {-lls_epoch_disc.item()}\")\n",
    "                    \n",
    "            if print_loss:\n",
    "                print('-----------------------------------')\n",
    "        \n",
    "        if num_gen != 0:\n",
    "            rmse_iter_list_gen = [] # list used to contain average rmse in each iteration\n",
    "            lls_iter_list_gen = [] # list used to contain average log-likelihood in each iteration\n",
    "            # fix hyperparameter v and generator G, update discriminator D num_nc times\n",
    "            gen_iter = tqdm.notebook.tqdm(range(num_gen), desc='Train Generator', leave=False)\n",
    "            for j in gen_iter:\n",
    "                # Within each iteration, we will go over each minibatch of data\n",
    "                minibatch_iter_gen = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch-Gen\", leave=False)\n",
    "                rmse_batch_list_gen = [] # list used to store rmse in mini-batch\n",
    "                lls_batch_list_gen = [] # list used to store log-likelihood in mini-batch\n",
    "                # fix discriminator D and train hyperparameter v with generator G\n",
    "                for x_batch_gen, y_batch_gen in minibatch_iter_gen:\n",
    "                    x_batch_gen, y_batch_gen = x_batch_gen.to(device), y_batch_gen.to(device)\n",
    "                    with num_likelihood_samples(num_samples):\n",
    "                        optimizer.zero_grad()\n",
    "                        trace_layer_list_gen = []\n",
    "                        norm_layer_list_gen = []\n",
    "                        u_layer_list_gen = []\n",
    "                        fiu_layer_list_gen = []\n",
    "                        glogpfu_layer_list_gen = []\n",
    "                        output_batch_gen, trace_layer_list_gen, norm_layer_list_gen, u_layer_list_gen, fiu_layer_list_gen, glogpfu_layer_list_gen = model.forward(\n",
    "                                                                            (x_batch_gen, True, False, False, trace_layer_list_gen, \n",
    "                                                                            norm_layer_list_gen, u_layer_list_gen, fiu_layer_list_gen, \n",
    "                                                                            glogpfu_layer_list_gen))\n",
    "                        trace_layer_tensor_gen = torch.stack(trace_layer_list_gen, dim=0)\n",
    "                        trace_loss_gen = trace_layer_tensor_gen.sum(0)\n",
    "\n",
    "                        norm_layer_tensor_gen = torch.stack(norm_layer_list_gen, dim=0)\n",
    "                        norm_loss_gen = norm_layer_tensor_gen.sum(0)\n",
    "                        norm_loss_gen = lamda * norm_loss_gen # need to negativate this loss\n",
    "\n",
    "                        glogpfu_tensor_gen = torch.stack(glogpfu_layer_list_gen, dim=0)\n",
    "                        glogpu_loss_gen = glogpfu_tensor_gen.sum(0)\n",
    "                        glogpu_loss_gen = -1 * glogpu_loss_gen  # occur wrong when update generator G\n",
    "\n",
    "                        # the second term in the first loss term\n",
    "                        # call the API instead of setting our own function\n",
    "                        begin = time.time()\n",
    "                        glogpyffu_loss_gen = lamda_like * model.regression_likelihood_loss(y_batch_gen, \n",
    "                                                output_batch_gen, u_layer_list_gen, fiu_layer_list_gen, \n",
    "                                                    glogpfu_layer_list_gen)\n",
    "                        end = time.time()\n",
    "                        if print_time:\n",
    "                            print('Compute likelihood time:', str(end - begin), 's')\n",
    "                        # the first loss term\n",
    "                        score_loss_gen = glogpu_loss_gen + glogpyffu_loss_gen\n",
    "                        # change here to form our own loss\n",
    "                        original_loss_gen = torch.abs(score_loss_gen + trace_loss_gen)\n",
    "                        loss_gen = original_loss_gen - norm_loss_gen\n",
    "\n",
    "                        if print_loss:\n",
    "                            print('GEN prior loss:', glogpu_loss_gen.item())\n",
    "                            print('GEN likelihood loss:', glogpyffu_loss_gen.item())\n",
    "                            print('GEN trace loss:', trace_loss_gen.item())\n",
    "                            print('GEN original loss:', original_loss_gen.item())\n",
    "                            print('GEN norm loss:', norm_loss_gen.item())\n",
    "                            print('GEN total loss:', loss_gen.item())\n",
    "\n",
    "                        torch.autograd.backward(loss_gen)\n",
    "                        optimizer.step()\n",
    "                        minibatch_iter_gen.set_postfix(loss=loss_gen.item())\n",
    "\n",
    "                        mean_batch_gen, covar_batch_gen = output_batch_gen\n",
    "                        '''\n",
    "                        f_batch_list_gen = []\n",
    "                        for t in range(0, covar_batch_gen.shape[0]):\n",
    "                            f_line_gen = torch.distributions.MultivariateNormal(loc=mean_batch_gen[i, :], \n",
    "                                                                                  covariance_matrix=covar_batch_gen[i, :, :]).rsample(\n",
    "                                torch.Size([])) # sample f from q(f)\n",
    "                            f_batch_list_gen.append(f_line_gen)\n",
    "                        f_batch_gen = torch.stack(f_batch_list_gen, dim=1)\n",
    "                        '''\n",
    "                        f_batch_gen = torch.distributions.MultivariateNormal(loc=mean_batch_gen, \n",
    "                                                                              covariance_matrix=covar_batch_gen).rsample(torch.Size([]))\n",
    "                        f_batch_gen = f_batch_gen.squeeze(0) # size: [batch_size]\n",
    "                        \n",
    "                        y_predict_gen = model.regression_likelihood(f_batch_gen)\n",
    "                        rmse_batch_gen = torch.mean(torch.pow(y_predict_gen - y_batch_gen, 2)).sqrt() # rmse value in mini-batch\n",
    "                        lls_batch_gen = model.test_regression_lls(y_predict_gen, y_batch_gen).mean(0)\n",
    "                        rmse_batch_list_gen.append(rmse_batch_gen)\n",
    "                        lls_batch_list_gen.append(lls_batch_gen)\n",
    "                \n",
    "\n",
    "                rmse_iter_gen = torch.stack(rmse_batch_list_gen, dim=0).mean(0) # rmse in one iteration\n",
    "                lls_iter_gen = torch.stack(lls_batch_list_gen, dim=0).mean(0) # log-likelihood in one iteration\n",
    "                if print_metric:\n",
    "                    print(f\"ITER GEN_RMSE: {rmse_iter_gen.item()}, ITER GEN_NLL: {-lls_iter_gen.item()}\")\n",
    "                rmse_iter_list_gen.append(rmse_iter_gen)\n",
    "                lls_iter_list_gen.append(lls_iter_gen)\n",
    "            \n",
    "\n",
    "            rmse_epoch_gen = torch.stack(rmse_iter_list_gen, dim=0).mean(0) # average rmse in one epoch\n",
    "            lls_epoch_gen = torch.stack(lls_iter_list_gen, dim=0).mean(0) # average log-likelihood in one epoch\n",
    "            if print_metric:\n",
    "                print(f\"EPOCH GEN_RMSE: {rmse_epoch_gen.item()}, EPOCH GEN_NLL: {-lls_epoch_gen.item()}\")            \n",
    "                    \n",
    "            if print_loss:\n",
    "                print('-----------------------------------')\n",
    "\n",
    "        if num_hyp != 0:\n",
    "            rmse_iter_list_hyp = [] # list used to contain average rmse in each iteration\n",
    "            lls_iter_list_hyp = [] # list used to contain average log-likelihood in each iteration\n",
    "            # fix hyperparameter v and generator G, update discriminator D num_nc times\n",
    "            hyp_iter = tqdm.notebook.tqdm(range(num_hyp), desc='Train Hyper-parameter', leave=False)\n",
    "            for j in hyp_iter:\n",
    "                # Within each iteration, we will go over each minibatch of data\n",
    "                minibatch_iter_hyp = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch-Hyper\", leave=False)\n",
    "                rmse_batch_list_hyp = [] # list used to store rmse in mini-batch\n",
    "                lls_batch_list_hyp = [] # list used to store log-likelihood in mini-batch\n",
    "                # fix discriminator D and train hyperparameter v with generator G\n",
    "                for x_batch_hyp, y_batch_hyp in minibatch_iter_hyp:\n",
    "                    x_batch_hyp, y_batch_hyp = x_batch_hyp.to(device), y_batch_hyp.to(device)\n",
    "                    with num_likelihood_samples(num_samples):\n",
    "                        optimizer.zero_grad()\n",
    "                        output_batch_hyp, _, _, _, _, _ = model.forward((x_batch_hyp, False, False, True, [], [], [], [], []))\n",
    "                        # call the API instead of setting our own function\n",
    "                        mean_batch_hyp, covar_batch_hyp = output_batch_hyp\n",
    "                        '''\n",
    "                        f_batch_list_hyp = []\n",
    "                        for t in range(0, covar_batch_hyp.shape[0]):\n",
    "                            f_line_hyp = torch.distributions.MultivariateNormal(loc=mean_batch_hyp[i, :], \n",
    "                                                                                  covariance_matrix=covar_batch_hyp[i, :, :]).rsample(\n",
    "                                torch.Size([])) # sample f from q(f)\n",
    "                            f_batch_list_hyp.append(f_line_hyp)\n",
    "                        f_batch_hyp = torch.stack(f_batch_list_hyp, dim=1)\n",
    "                        '''\n",
    "                        f_batch_hyp = torch.distributions.MultivariateNormal(loc=mean_batch_hyp, \n",
    "                                                                              covariance_matrix=covar_batch_hyp).rsample(torch.Size([]))\n",
    "                        f_batch_hyp = f_batch_hyp.squeeze(0) # size: [batch_size]\n",
    "                        \n",
    "                        y_predict_hyp = model.regression_likelihood(f_batch_hyp)\n",
    "                        loss_hyp = torch.mean(torch.pow(y_predict_hyp - y_batch_hyp, 2)).sqrt() # rmse value in mini-batch\n",
    "                        \n",
    "                        if print_loss:\n",
    "                                print('HYP total loss:', loss_hyp.item())\n",
    "                        torch.autograd.backward(loss_hyp)\n",
    "                        optimizer.step()\n",
    "                        minibatch_iter_hyp.set_postfix(loss=loss_hyp.item())\n",
    "                        \n",
    "                        rmse_batch_hyp = loss_hyp\n",
    "                        lls_batch_hyp = model.test_regression_lls(y_predict_hyp, y_batch_hyp).mean(0)\n",
    "                        rmse_batch_list_hyp.append(rmse_batch_hyp)\n",
    "                        lls_batch_list_hyp.append(lls_batch_hyp)\n",
    "                \n",
    "                rmse_iter_hyp = torch.stack(rmse_batch_list_hyp, dim=0).mean(0) # rmse in one iteration\n",
    "                lls_iter_hyp = torch.stack(lls_batch_list_hyp, dim=0).mean(0) # log-likelihood in one iteration\n",
    "                if print_metric:\n",
    "                    print(f\"ITER HYP_RMSE: {rmse_iter_hyp.item()}, ITER HYP_NLL: {-lls_iter_hyp.item()}\")\n",
    "                rmse_iter_list_hyp.append(rmse_iter_hyp)\n",
    "                lls_iter_list_hyp.append(lls_iter_hyp)\n",
    "\n",
    "            rmse_epoch_hyp = torch.stack(rmse_iter_list_hyp, dim=0).mean(0) # average rmse in one epoch\n",
    "            lls_epoch_hyp = torch.stack(lls_iter_list_hyp, dim=0).mean(0) # average log-likelihood in one epoch\n",
    "            if print_metric:\n",
    "                print(f\"EPOCH HYP_RMSE: {rmse_epoch_hyp.item()}, EPOCH HYP_NLL: {-lls_epoch_hyp.item()}\")\n",
    "            \n",
    "            if print_loss:\n",
    "                print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Train Disc and Gen----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e861a08c324dbda86f634f19d40bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New epoch!!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Discriminator:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Disc:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER DISC_RMSE: 1.597905158996582, ITER DISC_NLL: 2.202610492706299\n",
      "EPOCH DISC_RMSE: 1.597905158996582, EPOCH DISC_NLL: 2.202610492706299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Generator:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Gen:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER GEN_RMSE: 1.6173499822616577, ITER GEN_NLL: 2.236367702484131\n",
      "EPOCH GEN_RMSE: 1.6173499822616577, EPOCH GEN_NLL: 2.236367702484131\n",
      "----------Train Hyp----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d05bc0bfbc4d7cb175db49936ee1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New epoch!!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Hyper-parameter:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Hyper:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER HYP_RMSE: 1.595822811126709, ITER HYP_NLL: 2.2999420166015625\n",
      "EPOCH HYP_RMSE: 1.595822811126709, EPOCH HYP_NLL: 2.2999420166015625\n",
      "Training stage finish!!!\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# training stage\n",
    "print('----------Train Disc and Gen----------')\n",
    "train(num_epochs=1, num_disc=1, num_gen=1, num_hyp=0, num_samples=10, lamda=100, lamda_like=1)\n",
    "print('----------Train Hyp----------')\n",
    "train(num_epochs=1, num_disc=0, num_gen=0, num_hyp=1, num_samples=10, lamda=100, lamda_like=1)\n",
    "print('Training stage finish!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1482781171798706, NLL: 2.356001615524292\n",
      "RMSE: 1.2388978004455566, NLL: 2.2036736011505127\n",
      "RMSE: 1.2384344339370728, NLL: 2.247401714324951\n",
      "RMSE: 1.2900049686431885, NLL: 2.410691499710083\n",
      "RMSE: 1.0954948663711548, NLL: 2.610196113586426\n",
      "RMSE: 1.2637896537780762, NLL: 2.4080896377563477\n",
      "RMSE: 1.2467528581619263, NLL: 2.7060325145721436\n",
      "RMSE: 1.2367781400680542, NLL: 2.2750627994537354\n",
      "RMSE: 1.2568289041519165, NLL: 2.654531955718994\n",
      "RMSE: 1.2341171503067017, NLL: 2.366696834564209\n",
      "RMSE Mean: 1.2249376773834229, Std: 0.05818536505103111\n"
     ]
    }
   ],
   "source": [
    "test_itr = 10 # test iteration to get average metric and standard error\n",
    "\n",
    "# evaluate stage\n",
    "model.eval()\n",
    "\n",
    "rmse_list = []\n",
    "for r in range(0, test_itr):\n",
    "    predictive_means, test_lls = model.predict(test_loader)\n",
    "\n",
    "    rmse = torch.mean(torch.pow(predictive_means - test_y, 2)).sqrt()\n",
    "    # Since NLL has different ways to compute, we only compare the RMSE metric\n",
    "    print(f\"RMSE: {rmse.item()}, NLL: {-test_lls.mean().item()}\")\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "rmse_vector = torch.stack(rmse_list, dim=0)\n",
    "rmse_mean = rmse_vector.mean(0)\n",
    "rmse_std = rmse_vector.std(0)\n",
    "print(f\"RMSE Mean: {rmse_mean.item()}, Std: {rmse_std.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dushian",
   "language": "python",
   "name": "dushian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

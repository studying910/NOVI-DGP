{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import math\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.variational import VariationalStrategy\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:6' # change here to tune the device we use\n",
    "else:\n",
    "    device = 'cpu'\n",
    "# device = 'cpu'\n",
    "\n",
    "# load data and pre-processing\n",
    "# regression: UCI dataset, classification: Image dataset\n",
    "dataset = 'cifar10' # set here to determine to train which dataset\n",
    "if dataset == 'red_wine':\n",
    "    # L=6, D=11, N=1599\n",
    "    data = pd.read_csv('data/red_wine.csv').values\n",
    "    X = torch.Tensor(data[:, :11])\n",
    "    y = torch.Tensor(data[:, 11]).add(-3).long()\n",
    "elif dataset == 'white_wine':\n",
    "    # L=7, D=11, N=4898\n",
    "    data = pd.read_csv('data/white_wine.csv').values\n",
    "    X = torch.Tensor(data[:, :11])\n",
    "    y = torch.Tensor(data[:, 11]).add(-3).long()\n",
    "elif dataset == 'cifar10':\n",
    "    # L=10, C=3. H=32, W=32\n",
    "    pass\n",
    "elif dataset == 'mnist':\n",
    "    # L=10, C=1, H=28, W=28, label: 0-9\n",
    "    pass\n",
    "elif dataset == 'fashion_mnist':\n",
    "    # L=10, C=1, H=28, W=28, label: 0-9\n",
    "    pass\n",
    "elif dataset == 'abalone':\n",
    "    # L=29, D=7, N=4177\n",
    "    data = pd.read_excel('data/abalone.xlsx', header=None).values\n",
    "    X = torch.Tensor(data[:, :8])\n",
    "    y = torch.Tensor(data[:, 8]).add(-1).long()\n",
    "elif dataset == 'wilt':\n",
    "    # L=2, D=5, N=4840\n",
    "    train_data = pd.read_csv('data/wilt_training.csv').values\n",
    "    test_data = pd.read_csv('data/wilt_test.csv').values\n",
    "    train_x = train_data[:, 1:]\n",
    "    test_x = test_data[:, 1:]\n",
    "    train_x = train_x - train_x.min(0)[0]  # X.min(0)[0]: min value of every feature\n",
    "    train_x = 2 * (train_x / train_x.max(0)[0]) - 1 # pre-preocess to [-1,1]\n",
    "    test_x = test_x - test_x.min(0)[0]  # X.min(0)[0]: min value of every feature\n",
    "    test_x = 2 * (test_x / test_x.max(0)[0]) - 1 # pre-preocess to [-1,1]\n",
    "    \n",
    "    train_x = torch.Tensor(train_x).contiguous()\n",
    "    test_x = torch.Tensor(test_x).contiguous()\n",
    "    train_y = torch.Tensor(train_data[:, 0]).long().contiguous()\n",
    "    test_y = torch.Tensor(test_data[:, 0]).long().contiguous()\n",
    "    \n",
    "# pre-processing\n",
    "if dataset == 'cifar10':\n",
    "    transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "elif dataset == 'mnist':\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(), \n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "elif dataset == 'fashion_mnist':\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(), \n",
    "        torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "'''    \n",
    "elif dataset == 'wilt':\n",
    "    pass\n",
    "elif dataset == 'red_wine' or 'white_wine' or 'abalone':\n",
    "    X = X - X.min(0)[0]  # X.min(0)[0]: min value of every feature\n",
    "    X = 2 * (X / X.max(0)[0]) - 1 # pre-preocess to [-1,1]\n",
    "\n",
    "    # split data\n",
    "    ratio = 0.9 # ratio to split train set\n",
    "    train_n = int(floor(ratio * len(X))) # split train set, ratio: 0.8\n",
    "    train_x = X[:train_n, :].contiguous()\n",
    "    train_y = y[:train_n].contiguous()\n",
    "\n",
    "    test_x = X[train_n:, :].contiguous()\n",
    "    test_y = y[train_n:].contiguous()\n",
    "        \n",
    "# move data to cuda or cpu\n",
    "\n",
    "if dataset == 'red_wine' or 'white_wine' or 'abalone' or 'wilt':\n",
    "    train_x, train_y, test_x, test_y = train_x.to(device), train_y.to(device), test_x.to(device), test_y.to(device)\n",
    "\n",
    "'''\n",
    "def get_parameter_number(model):\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "if dataset == 'red_wine':\n",
    "    loader_batch_size = 512\n",
    "elif dataset == 'white_wine':\n",
    "    loader_batch_size = 512\n",
    "elif dataset == 'cifar10':\n",
    "    loader_batch_size = 512\n",
    "elif dataset == 'mnist':\n",
    "    loader_batch_size = 128\n",
    "elif dataset == 'fashion_mnist':\n",
    "    loader_batch_size = 256\n",
    "elif dataset == 'abalone':\n",
    "    loader_batch_size = 1024\n",
    "elif dataset == 'wilt':\n",
    "    loader_batch_size = 1024\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, \n",
    "                                                 transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, \n",
    "                                                 transform=transform_test)\n",
    "elif dataset == 'mnist':\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=False, \n",
    "                                                 transform=transform)\n",
    "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=False, \n",
    "                                                 transform=transform)\n",
    "elif dataset == 'fashion_mnist':\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, \n",
    "                                                 transform=transform)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, \n",
    "                                                 transform=transform)\n",
    "'''\n",
    "elif dataset == 'red_wine' or 'white_wine' or 'abalone' or 'wilt':\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "'''\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader1 = DataLoader(train_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_time = False # set True to print module time\n",
    "print_norm = False # set True to print norm of f_u (the perfect value of norm after training: 0)\n",
    "print_metric = True # set True to print metric per epoch\n",
    "print_param = True # set True to print parameter size of each network\n",
    "print_loss = False # set True to print each loss during training\n",
    "sample_times = 1 # Sample k times to evaluate the expectation over q(u)\n",
    "concat_type = False # set True to concat e with z in generator G\n",
    "expect_mean = True # Set True to compute mean of q(f) by average over q(u)\n",
    "\n",
    "class ToyDeepGPHiddenLayer(DeepGPLayer):  # input_dims: size of feature dim\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=128, mean_type='constant', stein=False, noise_add=True,\n",
    "                 noise_share=False, noise_dim=1, multi_head=False, vector_type=False):\n",
    "        # TODO: adjust the size of inducing_points to [inducing_size, input_dim] for each layer\n",
    "        if stein is False:\n",
    "            if output_dims is None:\n",
    "                inducing_points = torch.randn((num_inducing, input_dims), requires_grad=True) # sample from gaussian dist\n",
    "                batch_shape = torch.Size([])\n",
    "            else: # inducing_points-dim correspond to: output_dims, num_inducing and input_dims\n",
    "                inducing_points = torch.randn((output_dims, num_inducing, input_dims), requires_grad=True)\n",
    "                batch_shape = torch.Size([output_dims])\n",
    "        else:\n",
    "            if output_dims is None:\n",
    "                inducing_points = torch.randn((num_inducing, input_dims), requires_grad=True) # sample from gaussian dist\n",
    "                batch_shape = torch.Size([])\n",
    "            else: # inducing_points-dim correspond to: [num_inducing, input_dims]\n",
    "                inducing_points = torch.randn((num_inducing, input_dims), requires_grad=True)\n",
    "                batch_shape = torch.Size([output_dims])\n",
    "        \n",
    "        self.input_dim = input_dims\n",
    "        self.output_dim = output_dims\n",
    "        self.inducing_size = num_inducing\n",
    "\n",
    "        self.noise_add = noise_add\n",
    "        self.noise_share = noise_share\n",
    "        self.noise_dim = noise_dim # get this value from the outer model\n",
    "        if self.noise_dim is None:\n",
    "            self.noise_dim = 32\n",
    "        self.noise_add = noise_add\n",
    "        if self.noise_add:\n",
    "            self.concat_type = concat_type\n",
    "        else:\n",
    "            self.concat_type = False # only can set True when adding noise\n",
    "\n",
    "        # self.share_noise = share_noise # noise shared by all layers\n",
    "        self.multi_head = multi_head\n",
    "        # self.transformed_noise = transformed_noise\n",
    "        self.learn_inducing_locations = True # Set True to treat location of z as parameter\n",
    "        \n",
    "        self.hutch_times = 1 # number for hutchison estimation of trace\n",
    "        self.bottleneck_trick = True # Set True to introduce decomposition of jacobian to reduce variance\n",
    "        self.rademacher_type = False # Set True to sample from Rademecher dist to compute trace\n",
    "        self.diagonal_type = True # Set True to use diagonal kernel matrix to approximate fully kernel matrix\n",
    "        self.vector_type = vector_type # Set True to generate u using vector-based network\n",
    "        \n",
    "        self.print_time = print_time\n",
    "        self.print_param = print_param\n",
    "        self.device = device\n",
    "        self.sample_times = sample_times # Sample k times to evaluate the expectation over q(u)\n",
    "        self.expect_mean = expect_mean\n",
    "        \n",
    "        variational_distribution = None\n",
    "\n",
    "        variational_strategy = VariationalStrategy( # transform q(U) to q(F)\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            stein_type=stein,\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "\n",
    "        super(ToyDeepGPHiddenLayer, self).__init__(variational_strategy, input_dims, output_dims)        \n",
    "        self.variational_strategy = variational_strategy\n",
    "\n",
    "    def forward(self, x):\n",
    "        return None\n",
    "    \n",
    "    # will be called when run the forward function of class DeepGP\n",
    "    def __call__(self, x, train_type=True, disc_type=True, hyp_type=True, trace_layer_list=None, norm_layer_list=None, u_layer_list=None,\n",
    "                 fiu_layer_list=None, glogpfu_layer_list=None, shared_list=None, transformed_list=None, *other_inputs,\n",
    "                 **kwargs): \n",
    "        \"\"\"\n",
    "        Overriding __call__ isn't strictly necessary, but it lets us add concatenation based skip connections\n",
    "        easily.\n",
    "        \"\"\"\n",
    "        return super().__call__(x, are_samples=False, train_type=train_type, disc_type=disc_type,\n",
    "                                hyp_type=hyp_type, trace_layer_list=trace_layer_list, norm_layer_list=norm_layer_list,\n",
    "                                u_layer_list=u_layer_list, fiu_layer_list=fiu_layer_list, \n",
    "                                glogpfu_layer_list=glogpfu_layer_list, shared_list=shared_list,\n",
    "                               transformed_list=transformed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_dims = 10  # number of output_dim in hidden layer\n",
    "vector_type = True # Set True to generate u using vector-based network\n",
    "learn_likelihood_covariance = True # Set True to treat the covariance of likelihood as parameter\n",
    "if dataset == 'cifar10':\n",
    "    task_dim = 10\n",
    "    train_x_shape = 64 # carefully examine this term if self.feature_extractor is modified\n",
    "elif dataset == 'mnist':\n",
    "    task_dim = 10\n",
    "    train_x_shape = 28 * 28 # carefully examine this term if self.feature_extractor is modified\n",
    "elif dataset == 'fashion_mnist':\n",
    "    task_dim = 10\n",
    "    train_x_shape = 8 * 8 # carefully examine this term if self.feature_extractor is modified\n",
    "elif dataset == 'red_wine':\n",
    "    task_dim = 6\n",
    "    train_x_shape = train_x.shape[-1]\n",
    "elif dataset == 'white_wine':\n",
    "    task_dim = 7\n",
    "    train_x_shape = train_x.shape[-1]\n",
    "elif dataset == 'abalone':\n",
    "    task_dim = 29\n",
    "    train_x_shape = train_x.shape[-1]\n",
    "elif dataset == 'wilt':\n",
    "    task_dim = 2\n",
    "    train_x_shape = train_x.shape[-1]\n",
    "        \n",
    "# we can fine-tune it to obtain a better result\n",
    "if dataset == 'red_wine':\n",
    "    noise_dim = 200\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'white_wine':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'cifar10':\n",
    "    noise_dim = 200\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'mnist':\n",
    "    noise_dim = 200\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'fashion_mnist':\n",
    "    noise_dim = 200\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'abalone':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "elif dataset == 'wilt':\n",
    "    noise_dim = 32\n",
    "    num_layer_inducing = 128\n",
    "\n",
    "'''   \n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.maxpool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.maxpool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc0 = nn.Linear(512, 256)\n",
    "        self.bn0 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.bn11 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.bn11(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        return x\n",
    "        \n",
    "'''\n",
    "class ResNet18FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=64):\n",
    "        super(ResNet18FeatureExtractor, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False)\n",
    "        self.resnet.maxpool = nn.MaxPool2d(1, 1, 0)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.resnet.fc(x)\n",
    "        return x\n",
    "Extractor = ResNet18FeatureExtractor().to(device)\n",
    "state_dict = torch.load('ResNet_trained_extractor.pth')\n",
    "Extractor.load_state_dict(state_dict)\n",
    "\n",
    "        \n",
    "class DeepGP(DeepGP): # define the noise outside the layer\n",
    "    def __init__(self, train_x_shape, stein=False, noise_add=True, noise_share=False, multi_head=False): # L=2\n",
    "        if noise_add is False and noise_share:\n",
    "            raise ValueError('Only can share noise across layer when noise is added')\n",
    "        if noise_share is False and multi_head:\n",
    "            raise ValueError('Only can use Multi-head Mechanism when noise is shared across layer')\n",
    "        super().__init__()\n",
    "        self.vector_type = vector_type\n",
    "        self.noise_add = noise_add\n",
    "        self.noise_share = noise_share # share the noise across layer or not\n",
    "        # fine-tune this term or directly learn as a prior\n",
    "        self.noise_dim = noise_dim\n",
    "        self.multi_head = multi_head  # transform noise or not        \n",
    "        self.back_bone = nn.Sequential(\n",
    "            nn.Linear(in_features=self.noise_dim, out_features=32),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=32, out_features=self.noise_dim)\n",
    "        )\n",
    "        if self.multi_head and print_param:\n",
    "            print('Generator backbone:', get_parameter_number(self.back_bone))\n",
    "        \n",
    "        lls_sigma = torch.Tensor([0.1]) # Set sigma to be a small number\n",
    "        if learn_likelihood_covariance:\n",
    "            self.register_parameter(\"lls_sigma\", torch.nn.Parameter(lls_sigma))\n",
    "        else:\n",
    "            self.register_buffer(\"lls_sigma\", lls_sigma)\n",
    "        \n",
    "        if dataset == 'cifar10': # customize a feature_extractor for each image dataset\n",
    "            '''\n",
    "            self.feature_extractor = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten(), # output_dim: 3 * 32 * 32\n",
    "            )\n",
    "            '''\n",
    "            with torch.no_grad():\n",
    "                self.feature_extractor = Extractor.eval().to(device)\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "                #self.feature_extractor = ResNetFeatureExtractor.eval().to(device)\n",
    "            #self.feature_extractor=FeatureExtractor()\n",
    "        elif dataset == 'mnist' or 'fashion_mnist':\n",
    "            self.feature_extractor = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten(), # output_dim: 28 * 28\n",
    "            )\n",
    "        elif dataset == 'red_wine' or 'white_wine' or 'abalone' or 'wilt':\n",
    "            self.feature_extractor = nn.Flatten()\n",
    "\n",
    "        self.post_classification = nn.Sequential(\n",
    "            #nn.Linear(in_features=task_dim, out_features=task_dim),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.Linear(in_features=task_dim, out_features=task_dim),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.cross = nn.CrossEntropyLoss() # compute the classification loss to optimize hyper-parameter\n",
    "        \n",
    "        hidden_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=train_x_shape,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "        '''\n",
    "        hidden_layer2 = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer.output_dim,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )   \n",
    "        \n",
    "        \n",
    "        hidden_layer3 = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer2.output_dim,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        hidden_layer4 = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer3.output_dim,\n",
    "            output_dims=num_output_dims,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='linear',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "        '''\n",
    "\n",
    "        last_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer.output_dim,\n",
    "            output_dims=task_dim,\n",
    "            num_inducing=num_layer_inducing,\n",
    "            mean_type='constant',\n",
    "            stein=stein,\n",
    "            noise_add=noise_add,\n",
    "            noise_share=self.noise_share,\n",
    "            noise_dim=self.noise_dim,\n",
    "            multi_head=self.multi_head,\n",
    "            vector_type=self.vector_type,\n",
    "        )\n",
    "\n",
    "        self.hidden_layer = hidden_layer\n",
    "        #self.hidden_layer2 = hidden_layer2\n",
    "        #self.hidden_layer3 = hidden_layer3\n",
    "        # self.hidden_layer4 = hidden_layer4\n",
    "        self.last_layer = last_layer\n",
    "        \n",
    "        # register all networks here!!!\n",
    "        self.hidden_strategy_generator = self.hidden_layer.variational_strategy.generator\n",
    "        self.hidden_strategy_discriminator = self.hidden_layer.variational_strategy.discriminator\n",
    "        self.hidden_kernel_method = self.hidden_layer.variational_strategy.kernel_method\n",
    "        '''\n",
    "        self.hidden2_strategy_generator = self.hidden_layer2.variational_strategy.generator\n",
    "        self.hidden2_strategy_discriminator = self.hidden_layer2.variational_strategy.discriminator\n",
    "        self.hidden2_kernel_method = self.hidden_layer2.variational_strategy.kernel_method\n",
    "        \n",
    "        self.hidden3_strategy_generator = self.hidden_layer3.variational_strategy.generator\n",
    "        self.hidden3_strategy_discriminator = self.hidden_layer3.variational_strategy.discriminator\n",
    "        self.hidden3_kernel_method = self.hidden_layer3.variational_strategy.kernel_method\n",
    "        \n",
    "        self.hidden4_strategy_generator = self.hidden_layer4.variational_strategy.generator\n",
    "        self.hidden4_strategy_discriminator = self.hidden_layer4.variational_strategy.discriminator\n",
    "        self.hidden4_kernel_method = self.hidden_layer4.variational_strategy.kernel_method\n",
    "        '''\n",
    "        self.last_strategy_generator = self.last_layer.variational_strategy.generator\n",
    "        self.last_strategy_discriminator = self.last_layer.variational_strategy.discriminator\n",
    "        self.last_kernel_method = self.last_layer.variational_strategy.kernel_method\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # set train_type=False when evaluate, do not compute the loss to faster the process\n",
    "        # Generate noise here when noise is shared or transformed\n",
    "        if self.noise_add:\n",
    "            if self.noise_share:\n",
    "                if self.multi_head:\n",
    "                    if self.vector_type:\n",
    "                        shared_list = []\n",
    "                        transformed_list = []\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = 0.1*torch.randn(self.noise_dim, requires_grad=True).to(device)\n",
    "                            transformed_noise = self.back_bone(shared_noise)\n",
    "                            shared_list.append(shared_noise)\n",
    "                            transformed_list.append(transformed_noise)\n",
    "                    else:\n",
    "                        shared_list = []\n",
    "                        transformed_list = []\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = torch.randn((num_layer_inducing, self.noise_dim), requires_grad=True).to(device)\n",
    "                            transformed_noise = self.back_bone(shared_noise)\n",
    "                            shared_list.append(shared_noise)\n",
    "                            transformed_list.append(transformed_noise)\n",
    "                else:\n",
    "                    if self.vector_type:\n",
    "                        shared_list = []\n",
    "                        transformed_list = None\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = torch.randn(self.noise_dim, requires_grad=True).to(device)\n",
    "                            shared_list.append(shared_noise)                        \n",
    "                    else:\n",
    "                        shared_list = []\n",
    "                        transformed_list = None\n",
    "                        for i in range(0, sample_times):\n",
    "                            shared_noise = torch.randn((num_layer_inducing, self.noise_dim), requires_grad=True).to(device)\n",
    "                            shared_list.append(shared_noise)  \n",
    "            else:\n",
    "                shared_list = None\n",
    "                transformed_list = None\n",
    "        else:\n",
    "            shared_list = None\n",
    "            transformed_list = None\n",
    "        inputs, train_type, disc_type, hyp_type, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = inputs\n",
    "        # determine to train backbone network and parameter sigma or not\n",
    "        if train_type:\n",
    "            if disc_type:\n",
    "                self.back_bone.requires_grad_(False)\n",
    "                self.lls_sigma.requires_grad_(False)\n",
    "                self.feature_extractor.requires_grad_(False)\n",
    "                self.post_classification.requires_grad_(False)\n",
    "            else:\n",
    "                self.back_bone.requires_grad_(True)\n",
    "                self.lls_sigma.requires_grad_(False)\n",
    "                self.feature_extractor.requires_grad_(False)\n",
    "                self.post_classification.requires_grad_(False)\n",
    "        else:\n",
    "            if hyp_type:\n",
    "                self.back_bone.requires_grad_(False)\n",
    "                self.lls_sigma.requires_grad_(True)\n",
    "                self.feature_extractor.requires_grad_(False)\n",
    "                self.post_classification.requires_grad_(True)\n",
    "            else: # use for test stage\n",
    "                self.back_bone.requires_grad_(False)\n",
    "                self.lls_sigma.requires_grad_(False)\n",
    "                self.feature_extractor.requires_grad_(False)\n",
    "                self.post_classification.requires_grad_(False)\n",
    "        \n",
    "        hidden_rep1, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer(\n",
    "                                                                            inputs, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        '''\n",
    "        hidden_rep2, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer2(\n",
    "                                                                            hidden_rep1, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        \n",
    "        hidden_rep3, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer3(\n",
    "                                                                            hidden_rep2, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        \n",
    "        hidden_rep4, trace_layer_list, norm_layer_list, u_layer_list, fiu_layer_list, glogpfu_layer_list = self.hidden_layer4(\n",
    "                                                                            hidden_rep3, train_type=train_type, disc_type=disc_type,\n",
    "                                                                           hyp_type=hyp_type, trace_layer_list=trace_layer_list,\n",
    "                                                                           norm_layer_list=norm_layer_list,\n",
    "                                                                           u_layer_list=u_layer_list,\n",
    "                                                                          fiu_layer_list=fiu_layer_list,\n",
    "                                                                          glogpfu_layer_list=glogpfu_layer_list,\n",
    "                                                                          shared_list=shared_list,\n",
    "                                                                          transformed_list=transformed_list)\n",
    "        '''\n",
    "        \n",
    "        # get a prob distribution, not a deterministic vector value\n",
    "        output = self.last_layer(hidden_rep1, train_type=train_type, disc_type=disc_type, hyp_type=hyp_type, \n",
    "                                 trace_layer_list=trace_layer_list, norm_layer_list=norm_layer_list, \n",
    "                                 u_layer_list=u_layer_list, fiu_layer_list=fiu_layer_list, \n",
    "                                 glogpfu_layer_list=glogpfu_layer_list, shared_list=shared_list, \n",
    "                                 transformed_list=transformed_list)\n",
    "        return output\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        correct_sum = 0\n",
    "        total_sum = 0\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            x_batch = self.feature_extractor.forward(x_batch)\n",
    "            output_batch, _, _, _, _, _ = self.forward((x_batch, False, False, True, [], [], [], [], []))\n",
    "            mean_batch, covar_batch = output_batch\n",
    "            f_batch_list = []\n",
    "            for t in range(0, task_dim):\n",
    "                f_batch = torch.distributions.MultivariateNormal(loc=mean_batch[t, :], covariance_matrix=covar_batch[t, :, :]).rsample(\n",
    "                        torch.Size([])) # sample f from q(f)\n",
    "                f_batch_list.append(f_batch)\n",
    "            f_batch_total = torch.stack(f_batch_list, dim=1) # size: [batch_size, task_dim]\n",
    "            preds = self.classification_likelihood(f_batch_total)\n",
    "            #print(preds)\n",
    "            #print(y_batch)\n",
    "            correct = (preds == y_batch).sum().item()\n",
    "            total = y_batch.size(0)\n",
    "            print('Batch Accuracy: {}%'.format(100 * correct / total))\n",
    "\n",
    "            correct_sum += correct\n",
    "            total_sum += total\n",
    "                \n",
    "\n",
    "        total_acc = 100 * correct_sum / total_sum \n",
    "        return total_acc\n",
    "    \n",
    "    def classification_likelihood_loss(self, y_batch, output_batch, u_layer_list, fiu_layer_list, glogpfu_layer_list):\n",
    "        mean_batch, covariance_batch = output_batch\n",
    "        f_batch_list = []\n",
    "        for t in range(0, task_dim):\n",
    "            f_batch = torch.distributions.MultivariateNormal(loc=mean_batch[t, :], \n",
    "                                                             covariance_matrix=covariance_batch[t, :, :]).rsample(torch.Size([]))\n",
    "            f_batch_list.append(f_batch)\n",
    "        f_batch_total = torch.stack(f_batch_list, dim=1) # size: [batch_size, task_dim]\n",
    "        point_batch = self.post_classification(f_batch_total) # size: [batch_size, task_dim]\n",
    "        noise_batch = torch.ones_like(f_batch_total).mul(1e-4).to(device) # remove it if training of classification is stable\n",
    "        point_batch = point_batch.add(noise_batch)\n",
    "        #f_batch_total = f_batch_total.add(noise_batch)\n",
    "        y_size = y_batch.size(0)\n",
    "        one_hot_batch = torch.zeros(y_size, task_dim).long().to(device)\n",
    "        one_hot_batch.scatter_(dim=1, \n",
    "                               index=y_batch.unsqueeze(dim=1), \n",
    "                               src=torch.ones(y_size, task_dim).long().to(device)) # [batch_size, task_size]\n",
    "        prob_batch = torch.matmul(point_batch.unsqueeze(1), one_hot_batch.float().unsqueeze(2))\n",
    "        prob_batch = prob_batch.squeeze(2).squeeze(1)\n",
    "        log_prob_batch = torch.log(prob_batch).sum(0) # get the final log-prob value\n",
    "        \n",
    "        glogpyf_layer_list = [] # use to contain gradient of likelihood\n",
    "        for l in range(0, len(u_layer_list)):\n",
    "            glogpyf_l_list = []\n",
    "            for k in range(0, sample_times):\n",
    "                if expect_mean:\n",
    "                    glogpyf_l = torch.autograd.grad(log_prob_batch, u_layer_list[l][k], retain_graph=True, create_graph=True)[0]\n",
    "                else:\n",
    "                    # Set expect_mean == False to generate q(f) only using the first element of u_layer_list\n",
    "                    glogpyf_l = torch.autograd.grad(log_prob_batch, u_layer_list[l][0], retain_graph=True, create_graph=True)[0]\n",
    "                glogpyf_l_list.append(glogpyf_l)\n",
    "            glogpyf_layer_list.append(glogpyf_l_list)\n",
    "        \n",
    "        glogpyffu_layer_list = []\n",
    "        # glogpyffu_layer_list: the second term in the first loss term in each layer\n",
    "        for l in range(0, len(u_layer_list)):\n",
    "            glogpyffu_l_list = []\n",
    "            for k in range(0, sample_times):\n",
    "                glogpyffu_l = torch.mm(glogpyf_layer_list[l][k].unsqueeze(0), fiu_layer_list[l][k].unsqueeze(1))\n",
    "                glogpyffu_l_list.append(glogpyffu_l)\n",
    "            glogpyffu_l_tensor = torch.stack(glogpyffu_l_list, dim=0)\n",
    "            glogpyffu_l_mean = glogpyffu_l_tensor.mean(0) # average over sample_times\n",
    "            glogpyffu_layer_list.append(glogpyffu_l_mean)\n",
    "        glogpyffu_tensor = torch.stack(glogpyffu_layer_list, dim=0)\n",
    "        glogpyffu_loss = glogpyffu_tensor.sum(0) # sum over all layers\n",
    "        \n",
    "        return glogpyffu_loss\n",
    "    \n",
    "    def classification_likelihood(self, f_batch):\n",
    "        # size of f_batch: [batch_size, task_dim]\n",
    "        # We return the integer of class number that we predict the image belongs to\n",
    "        #prob_batch = self.post_classification(f_batch)\n",
    "        #y_predict = torch.max(prob_batch, dim=1)[1] # return the index [0, L-1] of each data point\n",
    "        y_predict = torch.max(f_batch, dim=1)[1] # return the index [0, L-1] of each data point\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator backbone: {'Total': 13032, 'Trainable': 13032}\n",
      "Generator: {'Total': 56864, 'Trainable': 56864}\n",
      "Discriminator: {'Total': 42257, 'Trainable': 42257}\n",
      "Kernel method: {'Total': 66, 'Trainable': 66}\n",
      "Generator: {'Total': 49952, 'Trainable': 49952}\n",
      "Discriminator: {'Total': 42257, 'Trainable': 42257}\n",
      "Kernel method: {'Total': 12, 'Trainable': 12}\n",
      "Test whether you have cuda to run the process or not: True\n",
      "{'Total': 11406105, 'Trainable': 204441}\n"
     ]
    }
   ],
   "source": [
    "# NOTE: When strange error occur (especially wrong error line), restart the kernel\n",
    "# If multi_head=True, Transform shared noise by a network and concat with layer-specified inducing points\n",
    "model = DeepGP(train_x_shape, stein=True, noise_add=True, noise_share=True, multi_head=True)\n",
    "print('Test whether you have cuda to run the process or not:', torch.cuda.is_available())\n",
    "model = model.to(device)\n",
    "print(get_parameter_number(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_list = [] \n",
    "acc_train_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total': 11406105, 'Trainable': 204441}\n"
     ]
    }
   ],
   "source": [
    "print(get_parameter_number(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5a599a9148450593d627aa71c18755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New epoch!!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Discriminator:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Disc:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 100.0%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 97.65625%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 100.0%\n",
      "MINI-BATCH DISC_ACC: 97.65625%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 97.265625%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 100.0%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 97.265625%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 100.0%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 97.65625%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 97.65625%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 100.0%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 97.65625%\n",
      "MINI-BATCH DISC_ACC: 100.0%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 97.65625%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 100.0%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 97.265625%\n",
      "MINI-BATCH DISC_ACC: 98.046875%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 99.609375%\n",
      "MINI-BATCH DISC_ACC: 98.4375%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 98.828125%\n",
      "MINI-BATCH DISC_ACC: 99.21875%\n",
      "MINI-BATCH DISC_ACC: 98.75%\n",
      "ITER DISC_ACC: 98.988%\n",
      "EPOCH DISC_ACC: 98.988%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Generator:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Gen:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 97.65625%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.046875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.046875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 97.65625%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.046875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.046875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.046875%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.046875%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 98.046875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.046875%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 96.875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 97.65625%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 98.828125%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.4375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 100.0%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.609375%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 99.21875%\n",
      "MINI-BATCH GEN_ACC: 98.75%\n",
      "ITER GEN_ACC: 99.074%\n",
      "EPOCH GEN_ACC: 99.074%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Hyper-parameter:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Hyper:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 95.3125%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "ITER HYP_ACC: 99.142%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 89.453125%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 96.09375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 88.671875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 93.75%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Hyper:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 95.3125%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "ITER HYP_ACC: 99.062%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 89.84375%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 96.09375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 89.0625%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 93.75%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Hyper:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "ITER HYP_ACC: 99.078%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 89.453125%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 91.40625%\n",
      "Batch Accuracy: 96.09375%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 89.0625%\n",
      "Batch Accuracy: 95.703125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 93.75%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Hyper:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 95.3125%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.75%\n",
      "ITER HYP_ACC: 99.078%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 89.84375%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 96.09375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 90.234375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 89.0625%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 93.75%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Hyper:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "ITER HYP_ACC: 99.048%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 89.84375%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 96.09375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 89.0625%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 93.75%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Hyper:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.75%\n",
      "ITER HYP_ACC: 99.07%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 89.84375%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 96.09375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 88.28125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 93.75%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Hyper:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.75%\n",
      "ITER HYP_ACC: 99.068%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 91.40625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 89.84375%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 96.09375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 88.28125%\n",
      "Batch Accuracy: 95.703125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 93.75%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Hyper:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "ITER HYP_ACC: 99.068%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 91.40625%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 89.453125%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 96.09375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 88.671875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 93.75%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Hyper:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.09375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.5%\n",
      "ITER HYP_ACC: 99.054%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 90.234375%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 91.40625%\n",
      "Batch Accuracy: 96.09375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 95.703125%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 90.625%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 88.28125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 93.75%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch-Hyper:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 96.875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 97.65625%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 99.21875%\n",
      "MINI-BATCH HYP_ACC: 98.4375%\n",
      "MINI-BATCH HYP_ACC: 100.0%\n",
      "ITER HYP_ACC: 99.104%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 89.84375%\n",
      "Batch Accuracy: 90.234375%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 96.09375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 88.28125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 93.75%\n",
      "EPOCH HYP_ACC: 99.0772%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gpytorch.settings import num_likelihood_samples\n",
    "begin_sum = time.time()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "optimizer1 = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00025)\n",
    "optimizer_gen = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "#def train(num_epochs=3, num_disc=2, num_gen=2, num_hyp=10, num_samples=10, lamda=100, lamda_like=1):\n",
    "\n",
    "num_epochs = 1\n",
    "num_disc = 1 # use to train discriminator per epoch\n",
    "num_gen = 1# use to train generator per epoch\n",
    "num_hyp = 10 # use to train hyper-parameter per epoch\n",
    "num_samples = 10 # use to evaluate likelihood\n",
    "lamda = 100 # hyperparameter for norm loss\n",
    "lamda_like = 1 # hyperparameter for likelihood loss (will be deleted soon)\n",
    "\n",
    "if num_epochs == 0:\n",
    "    raise ValueError('At least one epoch need to be run!!!')\n",
    "\n",
    "epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter: # change here to add adversarial training\n",
    "    print('New epoch!!!')\n",
    "    if num_disc != 0:\n",
    "        correct_epoch_disc = 0 # use to calculate correct prediction in one epoch\n",
    "        total_epoch_disc = 0 # use to calculate total prediction in one epoch\n",
    "        # fix hyperparameter v and generator G, update discriminator D num_nc times\n",
    "        disc_iter = tqdm.notebook.tqdm(range(num_disc), desc='Train Discriminator', leave=False)\n",
    "        for j in disc_iter:\n",
    "            # Within each iteration, we will go over each minibatch of data\n",
    "            minibatch_iter_disc = tqdm.notebook.tqdm(train_loader1, desc=\"Minibatch-Disc\", leave=False)\n",
    "            correct_iter_disc = 0 # use to calculate correct prediction in one iteration\n",
    "            total_iter_disc = 0 # use to calculate total prediction in one iteration\n",
    "            for x_batch_disc, y_batch_disc in minibatch_iter_disc:\n",
    "                x_batch_disc, y_batch_disc = x_batch_disc.to(device), y_batch_disc.to(device)\n",
    "                with num_likelihood_samples(num_samples):\n",
    "                    optimizer.zero_grad()\n",
    "                    trace_layer_list_disc = []\n",
    "                    norm_layer_list_disc = []\n",
    "                    u_layer_list_disc = []\n",
    "                    fiu_layer_list_disc = []\n",
    "                    glogpfu_layer_list_disc = []\n",
    "                    # trace_layer_list: contain the final trace in each layer\n",
    "                    # norm_layer_list: contain the norm in each layer\n",
    "                    # u_layer_list: contain u in each layer\n",
    "                    # fiu_layer_list: contain f_u in each layer\n",
    "                    # glogpu_layer_list: contain dot product of gradient of logp_u with f_u in each layer\n",
    "                    # disc_type: determine update discriminator or generator\n",
    "                    x_batch_disc = model.feature_extractor.forward(x_batch_disc)\n",
    "                    output_batch_disc, trace_layer_list_disc, norm_layer_list_disc, u_layer_list_disc, fiu_layer_list_disc, glogpfu_layer_list_disc = model.forward(\n",
    "                        (x_batch_disc, True, True, False, trace_layer_list_disc, norm_layer_list_disc, u_layer_list_disc, fiu_layer_list_disc, glogpfu_layer_list_disc))\n",
    "                    # size of output:([1, batch_size], [1, batch_size, batch_size])\n",
    "                    # the second loss term\n",
    "                    trace_layer_tensor_disc = torch.stack(trace_layer_list_disc, dim=0)\n",
    "                    trace_loss_disc = trace_layer_tensor_disc.sum(0)  \n",
    "                    # the third loss term\n",
    "                    norm_layer_tensor_disc = torch.stack(norm_layer_list_disc, dim=0)\n",
    "                    norm_loss_disc = norm_layer_tensor_disc.sum(0)\n",
    "                    if print_norm:\n",
    "                        print('DISC Norm of f_u:', norm_loss_disc.item())\n",
    "                    norm_loss_disc = lamda * norm_loss_disc # need to negativate this loss \n",
    "                    # the first term in the first loss term \n",
    "                    glogpfu_tensor_disc = torch.stack(glogpfu_layer_list_disc, dim=0)\n",
    "                    glogpu_loss_disc = glogpfu_tensor_disc.sum(0)\n",
    "                    glogpu_loss_disc = -1 * glogpu_loss_disc\n",
    "                    # the second term in the first loss term\n",
    "                    # call the API instead of setting our own function\n",
    "                    begin = time.time()                        \n",
    "                    glogpyffu_loss_disc = lamda_like * model.classification_likelihood_loss(y_batch_disc, \n",
    "                                            output_batch_disc, u_layer_list_disc, fiu_layer_list_disc, \n",
    "                                                glogpfu_layer_list_disc)                            \n",
    "                    end = time.time()\n",
    "                    if print_time:\n",
    "                        print('Compute likelihood time:', str(end - begin), 's')\n",
    "                    # the first loss term\n",
    "                    score_loss_disc = glogpu_loss_disc + glogpyffu_loss_disc\n",
    "                    # change here to form our own loss\n",
    "                    original_loss_disc = torch.abs(score_loss_disc + trace_loss_disc)\n",
    "                    loss_disc = original_loss_disc - norm_loss_disc\n",
    "                    if print_loss:\n",
    "                        print('DISC prior loss:', glogpu_loss_disc.item())\n",
    "                        print('DISC likelihood loss:', glogpyffu_loss_disc.item())\n",
    "                        print('DISC trace loss:', trace_loss_disc.item())\n",
    "                        print('DISC original loss:', original_loss_disc.item())\n",
    "                        print('DISC norm loss:', norm_loss_disc.item())\n",
    "                        print('DISC total loss:', loss_disc.item())\n",
    "\n",
    "                    torch.autograd.backward(-loss_disc) # nevigate it to max this loss\n",
    "                    optimizer.step()\n",
    "                    minibatch_iter_disc.set_postfix(loss=loss_disc.item())\n",
    "\n",
    "                    mean_batch_disc, covar_batch_disc = output_batch_disc\n",
    "\n",
    "                    f_batch_list_disc = []\n",
    "                    for t in range(0, task_dim):\n",
    "                        f_line_disc = torch.distributions.MultivariateNormal(loc=mean_batch_disc[t, :], \n",
    "                                                                              covariance_matrix=covar_batch_disc[t, :, :]).rsample(\n",
    "                            torch.Size([])) # sample f from q(f)\n",
    "                        f_batch_list_disc.append(f_line_disc)\n",
    "                    f_batch_disc = torch.stack(f_batch_list_disc, dim=1) # size: [batch_size, task_dim]\n",
    "                    '''\n",
    "                    f_batch_disc = torch.distributions.MultivariateNormal(loc=mean_batch_disc, \n",
    "                                                                          covariance_matrix=covar_batch_disc).rsample(torch.Size([]))\n",
    "                    f_batch_disc = f_batch_disc.squeeze(0) # size: [batch_size]\n",
    "                    '''\n",
    "                    y_predict_disc = model.classification_likelihood(f_batch_disc)\n",
    "                    correct_disc = (y_predict_disc == y_batch_disc).sum().item()\n",
    "                    total_disc = y_batch_disc.size(0)\n",
    "                    acc_disc = 100 * correct_disc / total_disc\n",
    "                    if print_metric:\n",
    "                        print(f\"MINI-BATCH DISC_ACC: {acc_disc}%\")\n",
    "                    correct_iter_disc += correct_disc\n",
    "                    total_iter_disc += total_disc\n",
    "\n",
    "            acc_iter_disc = 100 * correct_iter_disc / total_iter_disc\n",
    "            if print_metric:\n",
    "                print(f\"ITER DISC_ACC: {acc_iter_disc}%\")\n",
    "            correct_epoch_disc += correct_iter_disc\n",
    "            total_epoch_disc += total_iter_disc\n",
    "\n",
    "        acc_epoch_disc = 100 * correct_epoch_disc / total_epoch_disc\n",
    "        if print_metric:\n",
    "            print(f\"EPOCH DISC_ACC: {acc_epoch_disc}%\")\n",
    "\n",
    "        if print_loss:\n",
    "            print('-----------------------------------')\n",
    "\n",
    "    if num_gen != 0:\n",
    "        correct_epoch_gen = 0 # use to calculate correct prediction in one epoch\n",
    "        total_epoch_gen = 0 # use to calculate total prediction in one epoch\n",
    "        # fix hyperparameter v and generator G, update discriminator D num_nc times\n",
    "        gen_iter = tqdm.notebook.tqdm(range(num_gen), desc='Train Generator', leave=False)\n",
    "        for j in gen_iter:\n",
    "            # Within each iteration, we will go over each minibatch of data\n",
    "            minibatch_iter_gen = tqdm.notebook.tqdm(train_loader1, desc=\"Minibatch-Gen\", leave=False)\n",
    "            correct_iter_gen = 0 # use to calculate correct prediction in one iteration\n",
    "            total_iter_gen = 0 # use to calculate total prediction in one iteration\n",
    "            # fix discriminator D and train hyperparameter v with generator G\n",
    "            for x_batch_gen, y_batch_gen in minibatch_iter_gen:\n",
    "                x_batch_gen, y_batch_gen = x_batch_gen.to(device), y_batch_gen.to(device)\n",
    "                with num_likelihood_samples(num_samples):\n",
    "                    optimizer_gen.zero_grad()\n",
    "                    trace_layer_list_gen = []\n",
    "                    norm_layer_list_gen = []\n",
    "                    u_layer_list_gen = []\n",
    "                    fiu_layer_list_gen = []\n",
    "                    glogpfu_layer_list_gen = []\n",
    "                    x_batch_gen = model.feature_extractor.forward(x_batch_gen)\n",
    "                    output_batch_gen, trace_layer_list_gen, norm_layer_list_gen, u_layer_list_gen, fiu_layer_list_gen, glogpfu_layer_list_gen = model.forward(\n",
    "                                                                        (x_batch_gen, True, False, False, trace_layer_list_gen, \n",
    "                                                                        norm_layer_list_gen, u_layer_list_gen, fiu_layer_list_gen, \n",
    "                                                                        glogpfu_layer_list_gen))\n",
    "                    trace_layer_tensor_gen = torch.stack(trace_layer_list_gen, dim=0)\n",
    "                    trace_loss_gen = trace_layer_tensor_gen.sum(0)\n",
    "\n",
    "                    norm_layer_tensor_gen = torch.stack(norm_layer_list_gen, dim=0)\n",
    "                    norm_loss_gen = norm_layer_tensor_gen.sum(0)\n",
    "                    norm_loss_gen = lamda * norm_loss_gen # need to negativate this loss\n",
    "\n",
    "                    glogpfu_tensor_gen = torch.stack(glogpfu_layer_list_gen, dim=0)\n",
    "                    glogpu_loss_gen = glogpfu_tensor_gen.sum(0)\n",
    "                    glogpu_loss_gen = -1 * glogpu_loss_gen  # occur wrong when update generator G\n",
    "\n",
    "                    # the second term in the first loss term\n",
    "                    # call the API instead of setting our own function\n",
    "                    begin = time.time()\n",
    "                    glogpyffu_loss_gen = lamda_like * model.classification_likelihood_loss(y_batch_gen, \n",
    "                                            output_batch_gen, u_layer_list_gen, fiu_layer_list_gen, \n",
    "                                                glogpfu_layer_list_gen)\n",
    "                    end = time.time()\n",
    "                    if print_time:\n",
    "                        print('Compute likelihood time:', str(end - begin), 's')\n",
    "                    # the first loss term\n",
    "                    score_loss_gen = glogpu_loss_gen + glogpyffu_loss_gen\n",
    "                    # change here to form our own loss\n",
    "                    original_loss_gen = torch.abs(score_loss_gen + trace_loss_gen)\n",
    "                    loss_gen = original_loss_gen - norm_loss_gen\n",
    "\n",
    "                    if print_loss:\n",
    "                        print('GEN prior loss:', glogpu_loss_gen.item())\n",
    "                        print('GEN likelihood loss:', glogpyffu_loss_gen.item())\n",
    "                        print('GEN trace loss:', trace_loss_gen.item())\n",
    "                        print('GEN original loss:', original_loss_gen.item())\n",
    "                        print('GEN norm loss:', norm_loss_gen.item())\n",
    "                        print('GEN total loss:', loss_gen.item())\n",
    "\n",
    "                    torch.autograd.backward(loss_gen)\n",
    "                    optimizer_gen.step()\n",
    "                    minibatch_iter_gen.set_postfix(loss=loss_gen.item())\n",
    "\n",
    "                    mean_batch_gen, covar_batch_gen = output_batch_gen\n",
    "\n",
    "                    f_batch_list_gen = []\n",
    "                    for t in range(0, task_dim):\n",
    "                        f_line_gen = torch.distributions.MultivariateNormal(loc=mean_batch_gen[t, :], \n",
    "                                                                              covariance_matrix=covar_batch_gen[t, :, :]).rsample(\n",
    "                            torch.Size([])) # sample f from q(f)\n",
    "                        f_batch_list_gen.append(f_line_gen)\n",
    "                    f_batch_gen = torch.stack(f_batch_list_gen, dim=1)\n",
    "                    '''\n",
    "                    f_batch_gen = torch.distributions.MultivariateNormal(loc=mean_batch_gen, \n",
    "                                                                          covariance_matrix=covar_batch_gen).rsample(torch.Size([]))\n",
    "                    f_batch_gen = f_batch_gen.squeeze(0) # size: [batch_size]\n",
    "                    '''\n",
    "                    y_predict_gen = model.classification_likelihood(f_batch_gen)\n",
    "                    correct_gen = (y_predict_gen == y_batch_gen).sum().item()\n",
    "                    total_gen = y_batch_gen.size(0)\n",
    "                    acc_gen = 100 * correct_gen / total_gen\n",
    "                    if print_metric:\n",
    "                        print(f\"MINI-BATCH GEN_ACC: {acc_gen}%\")\n",
    "                    correct_iter_gen += correct_gen\n",
    "                    total_iter_gen += total_gen\n",
    "\n",
    "            acc_iter_gen = 100 * correct_iter_gen / total_iter_gen\n",
    "            if print_metric:\n",
    "                print(f\"ITER GEN_ACC: {acc_iter_gen}%\")\n",
    "            correct_epoch_gen += correct_iter_gen\n",
    "            total_epoch_gen += total_iter_gen\n",
    "\n",
    "        acc_epoch_gen = 100 * correct_epoch_gen / total_epoch_gen\n",
    "        if print_metric:\n",
    "            print(f\"EPOCH GEN_ACC: {acc_epoch_gen}%\")\n",
    "\n",
    "        if print_loss:\n",
    "            print('-----------------------------------')\n",
    "\n",
    "    if num_hyp != 0:\n",
    "        correct_epoch_hyp = 0 # use to calculate correct prediction in one epoch\n",
    "        total_epoch_hyp = 0 # use to calculate total prediction in one epoch\n",
    "        # fix hyperparameter v and generator G, update discriminator D num_nc times\n",
    "        hyp_iter = tqdm.notebook.tqdm(range(num_hyp), desc='Train Hyper-parameter', leave=False)\n",
    "        for j in hyp_iter:\n",
    "            # Within each iteration, we will go over each minibatch of data\n",
    "            minibatch_iter_hyp = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch-Hyper\", leave=False)\n",
    "            correct_iter_hyp = 0 # use to calculate correct prediction in one iteration\n",
    "            total_iter_hyp = 0 # use to calculate total prediction in one iteration\n",
    "            # fix discriminator D and train hyperparameter v with generator G\n",
    "            for x_batch_hyp, y_batch_hyp in minibatch_iter_hyp:\n",
    "                x_batch_hyp, y_batch_hyp = x_batch_hyp.to(device), y_batch_hyp.to(device)\n",
    "                with num_likelihood_samples(num_samples):\n",
    "                    optimizer1.zero_grad()\n",
    "                    x_batch_hyp = model.feature_extractor.forward(x_batch_hyp)\n",
    "                   \n",
    "                    \n",
    "                    output_batch_hyp, _, _, _, _, _ = model.forward((x_batch_hyp, False, False, True, [], [], [], [], []))\n",
    "                    # call the API instead of setting our own function\n",
    "                    mean_batch_hyp, covar_batch_hyp = output_batch_hyp\n",
    "\n",
    "                    f_batch_list_hyp = []\n",
    "                    for t in range(0, task_dim):\n",
    "                        f_line_hyp = torch.distributions.MultivariateNormal(loc=mean_batch_hyp[t, :], \n",
    "                                                                              covariance_matrix=covar_batch_hyp[t, :, :]).rsample(\n",
    "                            torch.Size([])) # sample f from q(f)\n",
    "                        f_batch_list_hyp.append(f_line_hyp)\n",
    "                    f_batch_hyp = torch.stack(f_batch_list_hyp, dim=1)\n",
    "                    '''\n",
    "                    f_batch_hyp = torch.distributions.MultivariateNormal(loc=mean_batch_hyp, \n",
    "                                                                          covariance_matrix=covar_batch_hyp).rsample(torch.Size([]))\n",
    "                    f_batch_hyp = f_batch_hyp.squeeze(0) # size: [batch_size]\n",
    "                    '''\n",
    "                    #prob_hyp = model.post_classification(f_batch_hyp) # may remove softmax to use unnormalized score\n",
    "                    #loss_hyp = model.cross(prob_hyp, y_batch_hyp)\n",
    "                    model.feature_extractor.requires_grad_(True)\n",
    "                    loss_hyp = model.cross(f_batch_hyp, y_batch_hyp)\n",
    "\n",
    "                    if print_loss:\n",
    "                            print('HYP total loss:', loss_hyp.item())\n",
    "                    torch.autograd.backward(loss_hyp)\n",
    "                    optimizer1.step()\n",
    "                    minibatch_iter_hyp.set_postfix(loss=loss_hyp.item())\n",
    "\n",
    "                    y_predict_hyp = model.classification_likelihood(f_batch_hyp)\n",
    "                    #print(y_predict_hyp)\n",
    "                    #print(y_batch_hyp)\n",
    "                    correct_hyp = (y_predict_hyp == y_batch_hyp).sum().item()\n",
    "                    total_hyp = y_batch_hyp.size(0)\n",
    "                    acc_hyp = 100 * correct_hyp / total_hyp\n",
    "                    if print_metric:\n",
    "                        print(f\"MINI-BATCH HYP_ACC: {acc_hyp}%\")\n",
    "                    correct_iter_hyp += correct_hyp\n",
    "                    total_iter_hyp += total_hyp\n",
    "\n",
    "            acc_iter_hyp = 100 * correct_iter_hyp / total_iter_hyp\n",
    "            if print_metric:\n",
    "                print(f\"ITER HYP_ACC: {acc_iter_hyp}%\")\n",
    "            correct_epoch_hyp += correct_iter_hyp\n",
    "            total_epoch_hyp += total_iter_hyp\n",
    "            \n",
    "            # store test rmse each iteration\n",
    "            acc_test_hyp= model.predict(test_loader)\n",
    "\n",
    "            acc_test_list.append(acc_test_hyp)\n",
    "            # store test rmse each iteration\n",
    "            acc_train_list.append(acc_iter_hyp)\n",
    "\n",
    "\n",
    "        acc_epoch_hyp = 100 * correct_epoch_hyp / total_epoch_hyp\n",
    "        if print_metric:\n",
    "            print(f\"EPOCH HYP_ACC: {acc_epoch_hyp}%\")\n",
    "\n",
    "        if print_loss:\n",
    "            print('-----------------------------------')\n",
    "end_sum = time.time()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# training stage\n",
    "#print('----------Train Disc and Gen----------')\n",
    "#train(num_epochs=1, num_disc=0, num_gen=0, num_hyp=2, num_samples=10, lamda=100, lamda_like=1)\n",
    "#print('----------Train Hyp----------')\n",
    "#train(num_epochs=1, num_disc=1, num_gen=1, num_hyp=3, num_samples=10, lamda=2000, lamda_like=1)\n",
    "#print('Training stage finish!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 89.84375%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.1875%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 91.796875%\n",
      "Batch Accuracy: 91.40625%\n",
      "Batch Accuracy: 96.09375%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 94.140625%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.75%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.578125%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 92.96875%\n",
      "Batch Accuracy: 91.015625%\n",
      "Batch Accuracy: 93.359375%\n",
      "Batch Accuracy: 94.53125%\n",
      "Batch Accuracy: 88.28125%\n",
      "Batch Accuracy: 95.3125%\n",
      "Batch Accuracy: 94.921875%\n",
      "Batch Accuracy: 93.75%\n",
      "Total Accuracy: 93.22%\n"
     ]
    }
   ],
   "source": [
    "import gpytorch\n",
    "import math\n",
    "\n",
    "# evaluate stage\n",
    "model.eval()\n",
    "\n",
    "\n",
    "total_acc = model.predict(test_loader)\n",
    "print('Total Accuracy: {}%'.format(total_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write test acc to experiments/cifar10.xlsx\n"
     ]
    }
   ],
   "source": [
    "print('Write test acc to experiments/cifar10.xlsx')\n",
    "df = pd.DataFrame(acc_test_list, columns=['ACC'])\n",
    "df.to_excel('experiments/twonorm5_test.xlsx', index=False)\n",
    "df_train = pd.DataFrame(acc_train_list, columns=['ACC'])\n",
    "df_train.to_excel('experiments/twonorm5_train.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 43.61 MB\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'novi.pt')\n",
    "file_size = os.path.getsize('novi.pt')\n",
    "print(f\"Model size: {file_size / 1024 / 1024:.2f} MB\")\n",
    "            \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "# define data transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "'''\n",
    "transform_train = transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(), \n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(), \n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "'''    \n",
    "\n",
    "# load CIFAR-10 dataset\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# define data loaders\n",
    "train_loader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False)\n",
    "\n",
    "# define the model\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.maxpool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.maxpool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc0 = nn.Linear(512, 256)\n",
    "        self.bn0 = nn.BatchNorm1d(256)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.bn11 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn11(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Extractor, optimizer, criterion, train_loader, device):\n",
    "    Extractor.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = Extractor(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_acc += pred.eq(target.view_as(pred)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "\n",
    "    return train_loss, train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(Extractor, criterion, test_loader, device):\n",
    "    Extractor.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = Extractor(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            test_acc += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc /= len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = 'cuda:6' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    Extractor = FeatureExtractor().to(device)\n",
    "    optimizer = optim.Adam(Extractor.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(30):\n",
    "        train_loss, train_acc = train(Extractor, optimizer, criterion, train_loader, device)\n",
    "        test_loss, test_acc = test(Extractor, criterion, test_loader, device)\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.6f}, Test Loss: {test_loss:.6f}, Test Acc: {test_acc:.6f}')\n",
    "\n",
    "    return Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.011299, Train Acc: 0.509800, Test Loss: 0.009002, Test Acc: 0.605600\n",
      "Epoch 2: Train Loss: 0.007533, Train Acc: 0.663700, Test Loss: 0.007095, Test Acc: 0.687700\n",
      "Epoch 3: Train Loss: 0.006310, Train Acc: 0.720260, Test Loss: 0.006855, Test Acc: 0.694600\n",
      "Epoch 4: Train Loss: 0.005606, Train Acc: 0.750080, Test Loss: 0.007604, Test Acc: 0.680600\n",
      "Epoch 5: Train Loss: 0.005098, Train Acc: 0.776780, Test Loss: 0.005310, Test Acc: 0.767900\n",
      "Epoch 6: Train Loss: 0.004712, Train Acc: 0.793780, Test Loss: 0.004935, Test Acc: 0.788800\n",
      "Epoch 7: Train Loss: 0.004378, Train Acc: 0.808140, Test Loss: 0.005309, Test Acc: 0.773300\n",
      "Epoch 8: Train Loss: 0.004074, Train Acc: 0.821460, Test Loss: 0.004841, Test Acc: 0.798700\n",
      "Epoch 9: Train Loss: 0.003801, Train Acc: 0.832600, Test Loss: 0.004350, Test Acc: 0.816600\n",
      "Epoch 10: Train Loss: 0.003654, Train Acc: 0.840220, Test Loss: 0.003979, Test Acc: 0.828400\n",
      "Epoch 11: Train Loss: 0.003427, Train Acc: 0.848340, Test Loss: 0.004438, Test Acc: 0.808700\n",
      "Epoch 12: Train Loss: 0.003246, Train Acc: 0.858800, Test Loss: 0.003986, Test Acc: 0.835800\n",
      "Epoch 13: Train Loss: 0.003093, Train Acc: 0.863800, Test Loss: 0.003936, Test Acc: 0.832200\n",
      "Epoch 14: Train Loss: 0.002993, Train Acc: 0.867900, Test Loss: 0.003666, Test Acc: 0.845500\n",
      "Epoch 15: Train Loss: 0.002817, Train Acc: 0.876020, Test Loss: 0.003622, Test Acc: 0.850900\n",
      "Epoch 16: Train Loss: 0.002716, Train Acc: 0.881420, Test Loss: 0.003635, Test Acc: 0.847700\n",
      "Epoch 17: Train Loss: 0.002577, Train Acc: 0.887880, Test Loss: 0.003626, Test Acc: 0.849600\n",
      "Epoch 18: Train Loss: 0.002440, Train Acc: 0.891640, Test Loss: 0.003758, Test Acc: 0.847200\n",
      "Epoch 19: Train Loss: 0.002364, Train Acc: 0.895180, Test Loss: 0.003741, Test Acc: 0.846300\n",
      "Epoch 20: Train Loss: 0.002239, Train Acc: 0.901980, Test Loss: 0.003598, Test Acc: 0.857000\n",
      "Epoch 21: Train Loss: 0.002175, Train Acc: 0.902880, Test Loss: 0.003390, Test Acc: 0.859600\n",
      "Epoch 22: Train Loss: 0.002060, Train Acc: 0.908540, Test Loss: 0.003420, Test Acc: 0.867000\n",
      "Epoch 23: Train Loss: 0.002003, Train Acc: 0.911720, Test Loss: 0.003902, Test Acc: 0.846600\n",
      "Epoch 24: Train Loss: 0.001877, Train Acc: 0.917080, Test Loss: 0.003486, Test Acc: 0.862400\n",
      "Epoch 25: Train Loss: 0.001806, Train Acc: 0.920060, Test Loss: 0.003651, Test Acc: 0.859700\n",
      "Epoch 26: Train Loss: 0.001775, Train Acc: 0.921740, Test Loss: 0.003460, Test Acc: 0.866400\n",
      "Epoch 27: Train Loss: 0.001674, Train Acc: 0.925380, Test Loss: 0.003778, Test Acc: 0.861700\n",
      "Epoch 28: Train Loss: 0.001651, Train Acc: 0.926460, Test Loss: 0.003699, Test Acc: 0.862500\n",
      "Epoch 29: Train Loss: 0.001594, Train Acc: 0.928320, Test Loss: 0.003772, Test Acc: 0.863900\n",
      "Epoch 30: Train Loss: 0.001534, Train Acc: 0.932840, Test Loss: 0.003626, Test Acc: 0.864800\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trained_extractor = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_extractor.eval()\n",
    "for param in trained_extractor.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc0): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (bn11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(trained_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设您的已训练好的特征提取器名称为trained_extractor\n",
    "torch.save(trained_extractor.state_dict(), 'trained_extractor.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xujian",
   "language": "python",
   "name": "xujian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
